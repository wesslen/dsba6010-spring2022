---
title: Exam
date: "2022-04-04"
menu:
  resource:
    parent: Study guides
type: docs
weight: 1
bibliography: ../../static/bib/references.bib
editor_options:
  chunk_output_type: console
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<script src="/rmarkdown-libs/clipboard/clipboard.min.js"></script>
<link href="/rmarkdown-libs/xaringanExtra-clipboard/xaringanExtra-clipboard.css" rel="stylesheet" />
<script src="/rmarkdown-libs/xaringanExtra-clipboard/xaringanExtra-clipboard.js"></script>
<script>window.xaringanExtraClipboard(null, {"button":"Copy Code","success":"Copied!","error":"Press Ctrl+C to Copy"})</script>


<p>April 18 we’ll have our in-class exam. It will be closed notes and based on chapter lectures, slides, and materials. It will be a blend of multiple choice/true-false questions (very similar to lecture quizzes) and short open ended questions (e.g., reading comprehension or explaining code snippets similar to problem sets). The exam is worth 200 points.</p>
<p>The expected time to complete the exam will be 1 hour but students may have the entire class period (12pm - 2:45pm) to complete.</p>
<p>The exam will cover material from Chapters 1 - 13 in Statistical Rethinking, which is consistent with the material from Lectures 1 - 13.</p>
<div id="part-1-multiple-choicetrue-false-questions-100-points" class="section level2">
<h2>Part 1: Multiple choice/true-false questions (100 points)</h2>
<p>Half of the exam will be multiple choice and true-false questions from Lesson quizzes 1-8. Some questions will be repeated and some questions will be new.</p>
<p>To help you study, I have prepared a pdf file with all of the questions and answers from Lesson quizzes 1-8. <a href="https://dsba6010-spring2022.netlify.app/resource/course-lesson-quizzes.pdf">You can download the file here</a>.</p>
<p>Similar to the lesson quizzes, multiple choice questions will be worth 4 points and true-false questions will be worth 2 points.</p>
</div>
<div id="part-2-short-answer-questions-100-points" class="section level2">
<h2>Part 2: Short answer questions (100 points)</h2>
<p>There will also be 8-10 short answer questions. These will be motivated from Lectures 1-13 and chapter 1-13. Lecture materials are the most important with chapter readings helpful if you need a more in-depth discussion on lecture topics. Solutions to problem sets 1-7 may also be helpful to review.</p>
<p>Here is a list of the relevant topics from each of the 13 lectures that will be covered.</p>
<table>
<colgroup>
<col width="47%" />
<col width="52%" />
</colgroup>
<thead>
<tr class="header">
<th>Lecture</th>
<th>Topics</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Models, hypotheses, “drawing the Bayesian owl,” Science before Statistics, DAGs/Causal inference vs prediction</td>
</tr>
<tr class="even">
<td>2</td>
<td>Bayesian data analysis, Garden of Forking Data / Bayesian marbles example, Globe tossing example, Posterior to prediction / sampling posterior</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Linear regression, why Normal distribution?, language for modeling / specifying Bayesian models, height-weight example, basics of generative modeling, prior/posterior sampling, simulation-based validation</td>
</tr>
<tr class="even">
<td>4</td>
<td>Causes aren’t in the data, categorical/index variables in regression, contrasts, polynomial, splines</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Interpretation and creation of DAGs, four elemental confounds, marriage/divorce example, plant height-fungus example, post-treatment bias, collider bias, descendents</td>
</tr>
<tr class="even">
<td>6</td>
<td>Parent-grandparent education example, DAG thinking / marginal effects, Do calculus, backdoor criterion, examples of finding adjustment set, good vs bad controls examples, Table 2 fallacy</td>
</tr>
<tr class="odd">
<td>7</td>
<td>Leave one-out cross validation, regularization, importance sampling / PSIS, WAIC, model mis-selection, outliers and robust regression</td>
</tr>
<tr class="even">
<td>8</td>
<td>Four methods for estimating posterior considered in class, intuition for Markov Chain Monte Carlo and Hamiltonian MC, convergence diagnostics (trace/trank plots, Rhat, effective samples), determining good vs bad convergence</td>
</tr>
<tr class="odd">
<td>9</td>
<td>Berkeley admission example / related examples, GLM modeling, logistic vs binomial regression, post-stratification</td>
</tr>
<tr class="even">
<td>10</td>
<td>Continued discussion on GLM, hidden collider bias in admissions example, sensitivity analysis, how/why more parameters than observations, oceanic tool example, poisson regression,</td>
</tr>
<tr class="odd">
<td>11</td>
<td>Trolley problem example, ordered categories, endogenous selection, monotonic predictors, complex causal effects</td>
</tr>
<tr class="even">
<td>12</td>
<td>Models with memory / intro to multi-level models, coffeeshop example, tad pole/tank example, varying effects superstitions,</td>
</tr>
<tr class="odd">
<td>13</td>
<td>Clusters and features, Chimpanzee example, multi-level predictions, handling divergent transitions</td>
</tr>
</tbody>
</table>
</div>
