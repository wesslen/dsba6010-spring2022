[{"authors":null,"categories":null,"content":"  I have included a bunch of extra resources and guides related to Bayesian statistics, causal inference, R, data, and other relevant topics. Enjoy!\n","date":1638316800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1638316800,"objectID":"8939c748f3090c6f91bdac5d32db55ec","permalink":"https://dsba6010-spring2022.netlify.app/resource/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/","section":"resource","summary":"I have included a bunch of extra resources and guides related to Bayesian statistics, causal inference, R, data, and other relevant topics. Enjoy!","tags":null,"title":"Helpful resources","type":"docs"},{"authors":null,"categories":null,"content":"  Visit this section after you have finished the readings and lecture videos. It contains fully annotated R code and other supplementary information and it will be indispensable as you work on your problem sets and project.\n","date":1638316800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1638316800,"objectID":"00e8826988eea7dfc8b8047b4c0184ce","permalink":"https://dsba6010-spring2022.netlify.app/example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/","section":"example","summary":"Visit this section after you have finished the readings and lecture videos. It contains fully annotated R code and other supplementary information and it will be indispensable as you work on your problem sets and project.","tags":null,"title":"Code examples","type":"docs"},{"authors":null,"categories":null,"content":"Each class session has a set of required readings that you should complete before watching the lecture.\nEvery class session also has a YouTube video for each of the lecture sections.\n Begin the course    View all slides in new window  Download PDF of all slides\nThe slides are also embedded on each page. You can click in the slides and navigate through them with ← and →. If you type ? (or shift + /) while viewing the slides you can see a list of slide-specific commands (like f for fullscreen or p for presenter mode if you want to see my notes). --!","date":1638316800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1638316800,"objectID":"d5be68294f12f9cfecf81ad87009adc6","permalink":"https://dsba6010-spring2022.netlify.app/content/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/content/","section":"content","summary":"Each class session has a set of required readings that you should complete before watching the lecture.\nEvery class session also has a YouTube video for each of the lecture sections.","tags":null,"title":"Readings, lectures, and videos","type":"docs"},{"authors":null,"categories":null,"content":"   The main goals of this class are to help you design, critique, code, and run Bayesian statistical models. Each type of assignment in this class is designed to help you achieve one or more of these goals.\nWeekly check-in Every week, after you finish working through the content, I want to hear about what you learned and what questions you still have. Because the content in this course is flipped, these questions are crucial for our weekly in-class discussions.\nTo encourage engagement with the course content—and to allow me to collect the class’s questions each week—you’ll need to fill out a short response on Google Forms. This should be ≈150 words. That’s fairly short: there are ≈250 words on a typical double-spaced page in Microsoft Word (500 when single-spaced).\nThese check-ins are due by noon on the days we have class. This is so I can look through the responses and start structuring the discussion for the evening’s class.\nYou should answer these two questions each week:\nWhat were the three (3) most interesting or exciting things you learned from the session? Why? What were the three (3) muddiest or unclear things from the session this week? What are you still wondering about?  You can include more than three interesting or muddiest things, but you must include at least three. There should be six easily identifiable things in each check-in: three exciting things and three questions.\nI will grade these check-ins using a check system:\n ✔+: (11.5 points (115%) in gradebook) Response shows phenomenal thought and engagement with the course content. I will not assign these often. ✔: (10 points (100%) in gradebook) Response is thoughtful, well-written, and shows engagement with the course content. This is the expected level of performance. ✔−: (5 points (50%) in gradebook) Response is hastily composed, too short, and/or only cursorily engages with the course content. This grade signals that you need to improve next time. I will hopefully not assign these often.  Notice that is essentially a pass/fail or completion-based system. I’m not grading your writing ability, I’m not counting the exact number of words you’re writing, and I’m not looking for encyclopedic citations of every single reading to prove that you did indeed read everything. I’m looking for thoughtful engagement, three interesting things, and three questions. That’s all. Do good work and you’ll get a ✓.\nYou will submit these check-ins via Canvas\n Problem sets To practice writing R code, running inferential models, and thinking about causation, you will complete a series of problem sets.\nYou need to show that you made a good faith effort to work each question. I will not grade these in detail. The problem sets will be graded using a check system:\n ✔++: (110% in gradebook) Assignment is 100% completed. Every question was attempted and answered, and most answers are correct. Document is clean and easy to follow. Work is exceptional. I will not assign these often. ✔+: (100% in gradebook) Assignment is 70–99% complete and most answers are correct. This is the expected level of performance. ✔: (90% in gradebook) Completed provided solutions and made a valid attempt at the non-provided questions. ✔−: (50% in gradebook) Assignment is less than 50% complete and/or most answers are incorrect. This indicates that you need to improve next time. I will hopefully not assign these often. ✔−−: (0% in gradebook) Assignment was not turned in I will hopefully not assign these often.  You may (and should!) work together on the problem sets, but you must turn in your own answers. You cannot work in groups of more than three people, and you must note who participated in the group in your assignment.\n Evaluation assignments For your final project, you will conduct a pre-registered evaluation of a social program using synthetic data. To (1) give you practice with the principles of program evaluation, research design, measurement, and causal diagrams, and (2) help you with the foundation of your final project, you will complete a set of four evaluation-related assignments.\nIdeally these will become major sections of your final project. However, there is no requirement that the programs you use in these assignments must be the same as the final project. If, through these assignments, you discover that your initially chosen program is too simple, too complex, too boring, etc., you can change at any time.\nThese assignments will be graded using a check system:\n ✔+: (33 points (110%) in gradebook) Assignment is 100% completed. Every question was attempted and answered, and most answers are correct. Document is clean and easy to follow. Work is exceptional. I will not assign these often. ✔: (30 points (100%) in gradebook) Assignment is 70–99% complete and most answers are correct. This is the expected level of performance. ✔−: (15 points (50%) in gradebook) Assignment is less than 70% complete and/or most answers are incorrect. This indicates that you need to improve next time. I will hopefully not asisgn these often.   Exams There will be one exam in the course that covers most lectures materials.\nThe exam will be in-class, paper, and without notes.\nIt will largely consist of lecture check-in questions, lecture, chapter readings, and problem sets.\nIt will be largely multiple choice questions along with short answer questions.\n Final project At the end of the course, you will demonstrate your knowledge of Bayesian statistics and causal inference by completing a final project.\nComplete details for the final project are here.\nThere is no final exam. This project is your final exam.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"3aa23ffb1eb3dedbe4d8a9c2165e2c58","permalink":"https://dsba6010-spring2022.netlify.app/assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/","section":"assignment","summary":"The main goals of this class are to help you design, critique, code, and run Bayesian statistical models. Each type of assignment in this class is designed to help you achieve one or more of these goals.","tags":null,"title":"Assignment details","type":"docs"},{"authors":null,"categories":null,"content":"Welcome!\n","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638316800,"objectID":"57f097deeaed40241747cdb16346aa0e","permalink":"https://dsba6010-spring2022.netlify.app/content/01-course-intro/","publishdate":"2021-12-01T00:00:00Z","relpermalink":"/content/01-course-intro/","section":"content","summary":"Welcome!","tags":null,"title":"Class 1 - Course Intro","type":"docs"},{"authors":null,"categories":null,"content":"   You will do all of your work in this class with the open source (and free!) programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—R handles all the calculations and the actual statistics, while RStudio provides a nice interface for running R code.1\nRStudio.cloud R is free, but it can sometimes be a pain to install and configure. To make life easier, you can (and should!) use the free RStudio.cloud service initially, which lets you run a full instance of RStudio in your web browser. This means you won’t have to install anything on your computer to get started with R! We will have a shared class workspace in RStudio.cloud that will let you quickly copy templates for labs and problem sets.\nGo to https://rstudio.cloud/ and create an account. You’ll receive a link to join the shared class workspace separately. If you don’t get this link, let me know and I will invite you.\n RStudio on your computer RStudio.cloud is convenient, but it can be slow and it is not designed to be able to handle larger datasets, more complicated analysis, or fancier graphics. Over the course of the semester, you should wean yourself off of RStudio.cloud and install all these things locally. This is also important if you want to customize fonts, since RStudio.cloud has extremely limited support for fonts other than Helvetica.\nHere’s how you install all these things\nInstall R First you need to install R itself (the engine).\nGo to the CRAN (Collective R Archive Network)2 website: https://cran.r-project.org/\n Click on “Download R for XXX”, where XXX is either Mac or Windows:\n If you use macOS, scroll down to the first .pkg file in the list of files (in this picture, it’s R-4.0.0.pkg; as of right now, the current version is also 4.0.0) and download it.   - If you use Windows, click \u0026quot;base\u0026quot; (or click on the bolded \u0026quot;install R for the first time\u0026quot; link) and download it.  Double click on the downloaded file (check your Downloads folder). Click yes through all the prompts to install like any other program.\n If you use macOS, download and install XQuartz. You do not need to do this on Windows.\n   Install RStudio Next, you need to install RStudio, the nicer graphical user interface (GUI) for R (the dashboard). Once R and RStudio are both installed, you can ignore R and only use RStudio. RStudio will use R automatically and you won’t ever have to interact with it directly.\nGo to the free download location on RStudio’s website: https://www.rstudio.com/products/rstudio/download/#download The website should automatically detect your operating system (macOS or Windows) and show a big download button for it:  If not, scroll down a little to the large table and choose the version of RStudio that matches your operating system. Double click on the downloaded file (again, check your Downloads folder). Click yes through all the prompts to install like any other program.  Double click on RStudio to run it (check your applications folder or start menu).\n Install tidyverse R packages are easy to install with RStudio. Select the packages panel, click on “Install,” type the name of the package you want to install, and press enter.\nThis can sometimes be tedious when you’re installing lots of packages, though. The tidyverse, for instance, consists of dozens of packages (including ggplot2) that all work together. Rather than install each individually, you can install a single magical package and get them all at the same time.\nGo to the packages panel in RStudio, click on “Install,” type “tidyverse”, and press enter. You’ll see a bunch of output in the RStudio console as all the tidyverse packages are installed.\nNotice also that RStudio will generate a line of code for you and run it: install.packages(\"tidyverse\"). You can also just paste and run this instead of using the packages panel.\n Install tinytex When you knit to PDF, R uses a special scientific typesetting program named LaTeX (pronounced “lay-tek” or “lah-tex”; for goofy nerdy reasons, the x is technically the “ch” sound in “Bach”, but most people just say it as “k”—saying “layteks” is frowned on for whatever reason).\nLaTeX is neat and makes pretty documents, but it’s a huge program—the macOS version, for instance, is nearly 4 GB! To make life easier, there’s an R package named tinytex that installs a minimal LaTeX program and that automatically deals with differences between macOS and Windows.\nHere’s how to install tinytex so you can knit to pretty PDFs:\nUse the Packages in panel in RStudio to install tinytex like you did above with tidyverse. Alternatively, run install.packages(\"tinytex\") in the console. Run tinytex::install_tinytex() in the console. Wait for a bit while R downloads and installs everything you need. The end! You should now be able to knit to PDF.     These instructions were created by Andrew Heiss from his excellent Program Evaluation course website.↩︎\n It’s a goofy name, but CRAN is where most R packages—and R itself—lives.↩︎\n   ","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638316800,"objectID":"efb59c0882a965443ffcbafa3cd27ca6","permalink":"https://dsba6010-spring2022.netlify.app/resource/install/","publishdate":"2021-12-01T00:00:00Z","relpermalink":"/resource/install/","section":"resource","summary":"You will do all of your work in this class with the open source (and free!) programming language R. You will use RStudio as the main program to access R.","tags":null,"title":"Installing R, RStudio, tidyverse, and tinytex","type":"docs"},{"authors":null,"categories":null,"content":"  Chapter 1: The Golem of Prague Data\n  Chapter 2: The Garden of Forking Data\n  Chapter 3: Sampling the Imaginary\nLectures Lecture 1    Lecture 2    Comprehension questions What is Bayesian data analysis? How does it differ in its definition of uncertainty from Frequentist interpretations?   An approach to count all the ways data can happen according to assumptions.\n  Bayesian data analysis uses probabilities to describe uncertainty. Importantly, in Bayesian data analysis probabilities describe degrees of belief. In contrast, a frequentist interpretation of probabilities would be as the frequencies of events in very large samples.\n  This leads to frequentist uncertainty being premised on imaginary resampling of data—if we were to repeat the measurement many many times, we would end up collecting a list of values that will have some pattern to it. It means also that parameters and models cannot have probability distributions, only measurements can.\n   Classical (Frequentist) statistical tests were originally developed for what purposes?   They were originally developed (largely by Ronald Fisher) in the early 20th century for agricultural applications. They typically were for randomized experiments with large effects, in which measurement issues had been solved.\n  Such statistical tests produce inferences, not decisions.\n   What is the core problem with null hypothesis testing?   Models are not hypotheses; they are neither true or false. Models are \u0026ldquo;golems\u0026rdquo; that do as they are told.\n  Ideally should compare performance across models (model comparison).\n  Popper: test (attempt to falsify) research hypothesis. Use theory, make a falsifiable prediction, and test that; not that nothing happened (aka null hypothesis).\n   What are three ways in which cross-validation and information theory aid in model evaluation?   They provide useful expectations of predictive accuracy, rather than merely fit to sample. So they compare models where it matters.\n  They give us an estimate of the tendency of a model to overfit. This will help us to understand how models and data interact, which in turn helps us to design better models.\n  They help us to spot highly influential observations.\n   We will cover these topics in Lesson 8.   What are the benefits of using multilevel (aka hierarchical) models?   They acknowledge that, though each individual group might have its own model, one group can provide valuable information about another. That is, \u0026ldquo;let’s learn from one another while celebrating our individuality.\u0026quot;\n  We will cover these later in the course.\n   Feedback Go to this form and answer these three questions:\nWhat was the muddiest thing from class this lesson? What are you still wondering about? What was the clearest thing from class this lesson? What was the most exciting thing you learned?  I’ll compile the questions and review for class.\n","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638316800,"objectID":"d65ea7c14dc448fa0015127ce682790a","permalink":"https://dsba6010-spring2022.netlify.app/content/02-bayesian-inference/","publishdate":"2021-12-01T00:00:00Z","relpermalink":"/content/02-bayesian-inference/","section":"content","summary":"  Chapter 1: The Golem of Prague Data\n  Chapter 2: The Garden of Forking Data\n  Chapter 3: Sampling the Imaginary\n","tags":["Bayesian methods"],"title":"Class 2 - Bayesian Inference","type":"docs"},{"authors":null,"categories":null,"content":"   We’ll primarily use Richard McElreath’s rethinking R package. Installation can be a bit tricky so this guide will outline how to install as well as other packages that may be helpful in this class.\nrethinking package The package’s github repository https://github.com/rmcelreath/rethinking is the best source of information, especially the issues section where likely if you run into errors, you may find tips to help install.\nInstall rstan While this course uses R, the engine for running Bayesian is Stan. Stan is a C++ library that can be used in a variety of ways (R, Python, Julia, Command Line, etc.).\nTo get started, we’ll use it with rstan, which is the R package aligned to calling Stan.\nTo install it, follow these instructions: https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started\nYou can find more information about rstan https://mc-stan.org/users/interfaces/rstan.html or you can view the Stan User Guide.\nTo test whether you properly installed rstan, try running this (see original GitHub Gist):\nset.seed(123) y \u0026lt;- rbinom(30, size = 1, prob = 0.2016) # Fitting a simple binomial model using Stan library(rstan) ## Loading required package: StanHeaders ## Loading required package: ggplot2 ## rstan (Version 2.21.2, GitRev: 2e1f913d3ca3) ## For execution on a local, multicore CPU with excess RAM we recommend calling ## options(mc.cores = parallel::detectCores()). ## To avoid recompilation of unchanged Stan programs, we recommend calling ## rstan_options(auto_write = TRUE) model_string \u0026lt;- \u0026quot; data { int n; int y[n]; } parameters { real\u0026lt;lower=0, upper=1\u0026gt; theta; } model { y ~ bernoulli(theta); }\u0026quot; stan_samples \u0026lt;- stan(model_code = model_string, data = list(y = y, n = length(y)) ) ## ## SAMPLING FOR MODEL \u0026#39;f6f19f2978447ffaf66a6d87ed7010fd\u0026#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 7e-06 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.003863 seconds (Warm-up) ## Chain 1: 0.0036 seconds (Sampling) ## Chain 1: 0.007463 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL \u0026#39;f6f19f2978447ffaf66a6d87ed7010fd\u0026#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 3e-06 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.004009 seconds (Warm-up) ## Chain 2: 0.003755 seconds (Sampling) ## Chain 2: 0.007764 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL \u0026#39;f6f19f2978447ffaf66a6d87ed7010fd\u0026#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 1e-06 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.003936 seconds (Warm-up) ## Chain 3: 0.003987 seconds (Sampling) ## Chain 3: 0.007923 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL \u0026#39;f6f19f2978447ffaf66a6d87ed7010fd\u0026#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 0 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.00389 seconds (Warm-up) ## Chain 4: 0.004015 seconds (Sampling) ## Chain 4: 0.007905 seconds (Total) ## Chain 4: stan_samples ## Inference for Stan model: f6f19f2978447ffaf66a6d87ed7010fd. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## theta 0.28 0.00 0.08 0.14 0.22 0.27 0.33 0.45 1505 1 ## lp__ -19.52 0.02 0.72 -21.59 -19.68 -19.24 -19.06 -19.01 1686 1 ## ## Samples were drawn using NUTS(diag_e) at Wed Dec 15 18:08:41 2021. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). traceplot(stan_samples) plot(stan_samples) ## ci_level: 0.8 (80% intervals) ## outer_level: 0.95 (95% intervals) Did you get similar output?\n Install cmdstanr Like all things, Stan is going through changes and moving forward, rstan is falling out of favor and instead much more development will be used with cmdstanr rather than rstan (here’s a comparison).\nThe rethinking package works with either of these packages but it’s likely much better to use cmdstanr instead of stanr; therefore, Richard (and I) recommend installing both. https://mc-stan.org/cmdstanr/\nTo install cmdstanr, you’ll need to install that from github (non-CRAN library).\n# if you get an error, do you have devtools installed? devtools::install_github(\u0026quot;stan-dev/cmdstanr\u0026quot;) After you’ve installed it, if it’s your first time you’ll need to run:\n# see https://mc-stan.org/cmdstanr/reference/install_cmdstan.html cmdstanr::install_cmdstan() To check if it was installed correctly, run:\ncmdstanr::check_cmdstan_toolchain() ## The C++ toolchain required for CmdStan is setup properly! library(cmdstanr) ## This is cmdstanr version 0.4.0.9000 ## - Online documentation and vignettes at mc-stan.org/cmdstanr ## - CmdStan path set to: /Users/rhymenoceros/.cmdstan/cmdstan-2.28.2 ## - Use set_cmdstan_path() to change the path file \u0026lt;- file.path(cmdstan_path(), \u0026quot;examples\u0026quot;, \u0026quot;bernoulli\u0026quot;, \u0026quot;bernoulli.stan\u0026quot;) data_list \u0026lt;- list(N = 10, y = c(0,1,0,0,0,0,0,0,0,1)) mod \u0026lt;- cmdstan_model(file) fit \u0026lt;- mod$sample( data = data_list, seed = 123, chains = 4, parallel_chains = 4, refresh = 500 ) ## Running MCMC with 4 parallel chains... ## ## Chain 1 Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1 Iteration: 500 / 2000 [ 25%] (Warmup) ## Chain 1 Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1 Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1 Iteration: 1500 / 2000 [ 75%] (Sampling) ## Chain 1 Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2 Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2 Iteration: 500 / 2000 [ 25%] (Warmup) ## Chain 2 Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2 Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2 Iteration: 1500 / 2000 [ 75%] (Sampling) ## Chain 2 Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3 Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3 Iteration: 500 / 2000 [ 25%] (Warmup) ## Chain 3 Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3 Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3 Iteration: 1500 / 2000 [ 75%] (Sampling) ## Chain 3 Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4 Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4 Iteration: 500 / 2000 [ 25%] (Warmup) ## Chain 4 Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4 Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4 Iteration: 1500 / 2000 [ 75%] (Sampling) ## Chain 4 Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1 finished in 0.0 seconds. ## Chain 2 finished in 0.0 seconds. ## Chain 3 finished in 0.0 seconds. ## Chain 4 finished in 0.0 seconds. ## ## All 4 chains finished successfully. ## Mean chain execution time: 0.0 seconds. ## Total execution time: 0.4 seconds. fit$summary() ## # A tibble: 2 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 lp__ -7.27 -6.98 0.725 0.317 -8.76 -6.75 1.00 1910. 1726. ## 2 theta 0.247 0.232 0.119 0.120 0.0784 0.461 1.00 1530. 1575.  Install rethinking Now with both rstan and cmdstanr, you can install rethinking:\ninstall.packages(c(\u0026quot;coda\u0026quot;,\u0026quot;mvtnorm\u0026quot;,\u0026quot;devtools\u0026quot;,\u0026quot;loo\u0026quot;,\u0026quot;dagitty\u0026quot;)) devtools::install_github(\u0026quot;rmcelreath/rethinking\u0026quot;) We can do a few functions to check to see if rethinking was installed correctly.\nlibrary(rethinking) ## Loading required package: parallel ## rethinking (Version 2.13) ## ## Attaching package: \u0026#39;rethinking\u0026#39; ## The following object is masked from \u0026#39;package:stats\u0026#39;: ## ## rstudent f \u0026lt;- alist( y ~ dnorm( mu , sigma ), mu ~ dnorm( 0 , 10 ), sigma ~ dexp( 1 ) ) fit \u0026lt;- quap( f , data=list(y=c(-1,1)) , start=list(mu=0,sigma=1) ) precis(fit) ## mean sd 5.5% 94.5% ## mu 0.0000000 0.5924240 -0.9468080 0.946808 ## sigma 0.8392882 0.3287434 0.3138927 1.364684 This first runs quap which uses quadratic approximation for fitting the posterior. This is what we’ll use for the first few weeks of the class before we use MCMC methods via Stan.\n# this uses cmdstanr version; this is recommended # you can set this permanently (i.e., don\u0026#39;t need the argument) by running set_ulam_cmdstan(TRUE) fit_stan \u0026lt;- ulam( f , data=list(y=c(-1,1)), cmdstan = TRUE ) ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 1 Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 1 Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 1 Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 1 Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 1 Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 1 Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 1 Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 1 Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 1 Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 1 Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 1 Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 1 finished in 0.0 seconds. precis(fit_stan) ## mean sd 5.5% 94.5% n_eff Rhat4 ## mu -0.08841271 1.6549093 -2.120005 1.957120 65.28501 1.0002767 ## sigma 1.62526571 0.9387858 0.686161 3.472576 54.75995 0.9982491   Optional but highly helpful Bayesian packages As Stan has grown in popularity, so too has Bayesian stats and a variety of package that work well with Stan.\nrstanarm First, I recommend installing rstanarm\ninstall.packages(\u0026quot;rstanarm\u0026quot;) Per its helpful vignette, the goal of the rstanarm package is to make Bayesian estimation routine for the most common regression models that applied researchers use. This will enable researchers to avoid the counter-intuitiveness of the frequentist approach to probability and statistics with only minimal changes to their existing R scripts.\nTo test whether the package was installed correctly, try this code:\nlibrary(rstanarm) ## Loading required package: Rcpp ## This is rstanarm version 2.21.1 ## - See https://mc-stan.org/rstanarm/articles/priors for changes to default priors! ## - Default priors may change, so it\u0026#39;s safest to specify priors, even if equivalent to the defaults. ## - For execution on a local, multicore CPU with excess RAM we recommend calling ## options(mc.cores = parallel::detectCores()) ## ## Attaching package: \u0026#39;rstanarm\u0026#39; ## The following objects are masked from \u0026#39;package:rethinking\u0026#39;: ## ## logit, se ## The following object is masked from \u0026#39;package:rstan\u0026#39;: ## ## loo fit \u0026lt;- stan_glm(mpg ~ ., data = mtcars) ## ## SAMPLING FOR MODEL \u0026#39;continuous\u0026#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 0.000178 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.78 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.126841 seconds (Warm-up) ## Chain 1: 0.134685 seconds (Sampling) ## Chain 1: 0.261526 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL \u0026#39;continuous\u0026#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 4e-06 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.121206 seconds (Warm-up) ## Chain 2: 0.102408 seconds (Sampling) ## Chain 2: 0.223614 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL \u0026#39;continuous\u0026#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 4e-06 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.12403 seconds (Warm-up) ## Chain 3: 0.124122 seconds (Sampling) ## Chain 3: 0.248152 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL \u0026#39;continuous\u0026#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 4e-06 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.126184 seconds (Warm-up) ## Chain 4: 0.118263 seconds (Sampling) ## Chain 4: 0.244447 seconds (Total) ## Chain 4: plot(fit)  bayesplot Another package that is handy is bayesplot, which provides helpful ways to visualize results from Stan models.\nTo install it, you’ll need to run:\ninstall.packages(\u0026quot;bayesplot\u0026quot;) You can then test it out running this code:\nlibrary(bayesplot) ## This is bayesplot version 1.8.1 ## - Online documentation and vignettes at mc-stan.org/bayesplot ## - bayesplot theme set to bayesplot::theme_default() ## * Does _not_ affect other ggplot2 plots ## * See ?bayesplot_theme_set for details on theme setting library(ggplot2) # assume you already have it installed posterior \u0026lt;- as.matrix(fit) plot_title \u0026lt;- ggtitle(\u0026quot;Posterior distributions\u0026quot;, \u0026quot;with medians and 80% intervals\u0026quot;) mcmc_areas(posterior, pars = c(\u0026quot;cyl\u0026quot;, \u0026quot;drat\u0026quot;, \u0026quot;am\u0026quot;, \u0026quot;wt\u0026quot;), prob = 0.8) + plot_title This can also provide helpful post-predictive checks (i.e., see model fitting), for example:\ncolor_scheme_set(\u0026quot;red\u0026quot;) ppc_dens_overlay(y = fit$y, yrep = posterior_predict(fit, draws = 50))  brms For most Bayesian research projects, brms has become the most popular R package. https://paul-buerkner.github.io/brms/\nIt combines the intuition of class R regression modeling (y ~ IV1 + IV1) with mixed effects modeling like lme4 syntax. This class we won’t use this package (maybe near the end of course) but it’s incredibly helpful to think about for your final project.\nLike other CRAN libraries, to install:\ninstall.packages(\u0026quot;brms\u0026quot;) You can then run Bayesian mixed effects modeling simply with:\n# tidybayes example: http://mjskay.github.io/tidybayes/articles/tidy-brms.html set.seed(5) n = 10 n_condition = 5 ABC = data.frame( condition = rep(c(\u0026quot;A\u0026quot;,\u0026quot;B\u0026quot;,\u0026quot;C\u0026quot;,\u0026quot;D\u0026quot;,\u0026quot;E\u0026quot;), n), response = rnorm(n * 5, c(0,1,2,1,-1), 0.5) ) library(brms) ## Loading \u0026#39;brms\u0026#39; package (version 2.16.3). Useful instructions ## can be found by typing help(\u0026#39;brms\u0026#39;). A more detailed introduction ## to the package is available through vignette(\u0026#39;brms_overview\u0026#39;). ## ## Attaching package: \u0026#39;brms\u0026#39; ## The following objects are masked from \u0026#39;package:rstanarm\u0026#39;: ## ## dirichlet, exponential, get_y, lasso, ngrps ## The following objects are masked from \u0026#39;package:rethinking\u0026#39;: ## ## LOO, stancode, WAIC ## The following object is masked from \u0026#39;package:rstan\u0026#39;: ## ## loo ## The following object is masked from \u0026#39;package:stats\u0026#39;: ## ## ar m = brm( response ~ (1|condition), data = ABC, prior = c( prior(normal(0, 1), class = Intercept), prior(student_t(3, 0, 1), class = sd), prior(student_t(3, 0, 1), class = sigma) ), backend = \u0026quot;cmdstanr\u0026quot;, # optional; if not specified, will use stanr control = list(adapt_delta = .99) ) ## Start sampling ## Running MCMC with 4 sequential chains... ## ## Chain 1 Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1 Iteration: 100 / 2000 [ 5%] (Warmup) ## Chain 1 Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1 Iteration: 300 / 2000 [ 15%] (Warmup) ## Chain 1 Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1 Iteration: 500 / 2000 [ 25%] (Warmup) ## Chain 1 Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1 Iteration: 700 / 2000 [ 35%] (Warmup) ## Chain 1 Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1 Iteration: 900 / 2000 [ 45%] (Warmup) ## Chain 1 Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1 Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1 Iteration: 1100 / 2000 [ 55%] (Sampling) ## Chain 1 Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1 Iteration: 1300 / 2000 [ 65%] (Sampling) ## Chain 1 Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1 Iteration: 1500 / 2000 [ 75%] (Sampling) ## Chain 1 Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1 Iteration: 1700 / 2000 [ 85%] (Sampling) ## Chain 1 Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1 Iteration: 1900 / 2000 [ 95%] (Sampling) ## Chain 1 Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1 finished in 0.4 seconds. ## Chain 2 Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2 Iteration: 100 / 2000 [ 5%] (Warmup) ## Chain 2 Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2 Iteration: 300 / 2000 [ 15%] (Warmup) ## Chain 2 Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2 Iteration: 500 / 2000 [ 25%] (Warmup) ## Chain 2 Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2 Iteration: 700 / 2000 [ 35%] (Warmup) ## Chain 2 Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2 Iteration: 900 / 2000 [ 45%] (Warmup) ## Chain 2 Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2 Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2 Iteration: 1100 / 2000 [ 55%] (Sampling) ## Chain 2 Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2 Iteration: 1300 / 2000 [ 65%] (Sampling) ## Chain 2 Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2 Iteration: 1500 / 2000 [ 75%] (Sampling) ## Chain 2 Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2 Iteration: 1700 / 2000 [ 85%] (Sampling) ## Chain 2 Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2 Iteration: 1900 / 2000 [ 95%] (Sampling) ## Chain 2 Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2 finished in 0.4 seconds. ## Chain 3 Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3 Iteration: 100 / 2000 [ 5%] (Warmup) ## Chain 3 Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3 Iteration: 300 / 2000 [ 15%] (Warmup) ## Chain 3 Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3 Iteration: 500 / 2000 [ 25%] (Warmup) ## Chain 3 Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3 Iteration: 700 / 2000 [ 35%] (Warmup) ## Chain 3 Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3 Iteration: 900 / 2000 [ 45%] (Warmup) ## Chain 3 Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3 Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3 Iteration: 1100 / 2000 [ 55%] (Sampling) ## Chain 3 Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3 Iteration: 1300 / 2000 [ 65%] (Sampling) ## Chain 3 Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3 Iteration: 1500 / 2000 [ 75%] (Sampling) ## Chain 3 Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3 Iteration: 1700 / 2000 [ 85%] (Sampling) ## Chain 3 Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3 Iteration: 1900 / 2000 [ 95%] (Sampling) ## Chain 3 Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3 finished in 0.5 seconds. ## Chain 4 Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4 Iteration: 100 / 2000 [ 5%] (Warmup) ## Chain 4 Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4 Iteration: 300 / 2000 [ 15%] (Warmup) ## Chain 4 Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4 Iteration: 500 / 2000 [ 25%] (Warmup) ## Chain 4 Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4 Iteration: 700 / 2000 [ 35%] (Warmup) ## Chain 4 Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4 Iteration: 900 / 2000 [ 45%] (Warmup) ## Chain 4 Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4 Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4 Iteration: 1100 / 2000 [ 55%] (Sampling) ## Chain 4 Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4 Iteration: 1300 / 2000 [ 65%] (Sampling) ## Chain 4 Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4 Iteration: 1500 / 2000 [ 75%] (Sampling) ## Chain 4 Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4 Iteration: 1700 / 2000 [ 85%] (Sampling) ## Chain 4 Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4 Iteration: 1900 / 2000 [ 95%] (Sampling) ## Chain 4 Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4 finished in 0.4 seconds. ## ## All 4 chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 2.0 seconds. m ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: response ~ (1 | condition) ## Data: ABC (Number of observations: 50) ## Draws: 4 chains, each with iter = 1000; warmup = 0; thin = 1; ## total post-warmup draws = 4000 ## ## Group-Level Effects: ## ~condition (Number of levels: 5) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 1.15 0.42 0.61 2.19 1.00 890 1168 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.49 0.45 -0.46 1.36 1.00 928 1150 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.56 0.06 0.46 0.69 1.00 1700 1671 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Soloman Kurz has an amazing reproduction of our textbook but translated into brms as well as tidyverse: Statistical rethinking with brms, ggplot2, and the tidyverse: Second edition\n tidybayes and ggdist Last, tidybayes is in incredibly helpful package that combines Bayesian statistics from a tidy perspective that underpins the tidyverse.\nIn addition, tidybayes’ sister package, ggdist, enables incredibly powerful uncertainty visualizations via ggplot2.\nTo install these packages, you can run:\ninstall.packages(\u0026#39;ggdist\u0026#39;) install.packages(\u0026#39;tidybayes\u0026#39;) library(ggdist) ## ## Attaching package: \u0026#39;ggdist\u0026#39; ## The following objects are masked from \u0026#39;package:brms\u0026#39;: ## ## dstudent_t, pstudent_t, qstudent_t, rstudent_t library(tidybayes) ## ## Attaching package: \u0026#39;tidybayes\u0026#39; ## ## The following objects are masked from \u0026#39;package:brms\u0026#39;: ## ## dstudent_t, pstudent_t, qstudent_t, rstudent_t library(magrittr) # part of tidyverse; install if you don\u0026#39;t have ## ## Attaching package: \u0026#39;magrittr\u0026#39; ## The following object is masked from \u0026#39;package:rstan\u0026#39;: ## ## extract library(modelr) # install if you don\u0026#39;t have; data_grid is from it ## ## Attaching package: \u0026#39;modelr\u0026#39; ## The following object is masked from \u0026#39;package:rethinking\u0026#39;: ## ## resample ABC %\u0026gt;% data_grid(condition) %\u0026gt;% add_epred_rvars(m) %\u0026gt;% ggplot(aes(dist = .epred, y = condition)) + stat_dist_dotsinterval(quantiles = 100) This provides posterior means using quantile dot plots, which are frequency-format uncertainty visualizations that have showed great promise in improving decision-making (Kay et al. 2016, Fernandes et al. 2018)\n tidybayes.rethinking There is even a tidybayes package that works specifically for the rethinking package. I encourage you to install this as we may\ndevtools::install_github(\u0026quot;mjskay/tidybayes.rethinking\u0026quot;) You can now run this:\nlibrary(tidybayes.rethinking) # bayesian regression wt (x) on mpg (y) m = quap(alist( mpg ~ dlnorm(mu, sigma), mu \u0026lt;- a + b*wt, c(a,b) ~ dnorm(0, 10), sigma ~ dexp(1) ), data = mtcars, start = list(a = 4, b = -1, sigma = 1) ) m %\u0026gt;% tidybayes::tidy_draws() %\u0026gt;% tidybayes::gather_variables() %\u0026gt;% median_qi() %\u0026gt;% ggplot(aes(y = .variable, x = .value, xmin = .lower, xmax = .upper)) + geom_pointinterval()   Package versions above When using R (or any open source language), it’s very important to keep links for package versions. I highly recommend running in Rmarkdown files this code at the end to timestamp what packages were used.\nsessionInfo() ## R version 4.1.1 (2021-08-10) ## Platform: aarch64-apple-darwin20 (64-bit) ## Running under: macOS Monterey 12.0.1 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] parallel stats graphics grDevices datasets utils methods ## [8] base ## ## other attached packages: ## [1] tidybayes.rethinking_3.0.0 modelr_0.1.8 ## [3] magrittr_2.0.1 tidybayes_3.0.1 ## [5] ggdist_3.0.1 brms_2.16.3 ## [7] bayesplot_1.8.1 rstanarm_2.21.1 ## [9] Rcpp_1.0.7 digest_0.6.29 ## [11] rethinking_2.13 cmdstanr_0.4.0.9000 ## [13] rstan_2.21.2 ggplot2_3.3.5 ## [15] StanHeaders_2.21.0-7 knitr_1.36 ## ## loaded via a namespace (and not attached): ## [1] minqa_1.2.4 colorspace_2.0-2 ellipsis_0.3.2 ## [4] ggridges_0.5.3 rsconnect_0.8.25 markdown_1.1 ## [7] base64enc_0.1-3 rstudioapi_0.13 farver_2.1.0 ## [10] svUnit_1.0.6 DT_0.20 fansi_0.5.0 ## [13] mvtnorm_1.1-3 bridgesampling_1.1-2 codetools_0.2-18 ## [16] splines_4.1.1 shinythemes_1.2.0 jsonlite_1.7.2 ## [19] nloptr_1.2.2.3 broom_0.7.9 shiny_1.7.1 ## [22] compiler_4.1.1 backports_1.3.0 assertthat_0.2.1 ## [25] Matrix_1.3-4 fastmap_1.1.0 cli_3.1.0 ## [28] later_1.3.0 htmltools_0.5.2 prettyunits_1.1.1 ## [31] tools_4.1.1 igraph_1.2.9 coda_0.19-4 ## [34] gtable_0.3.0 glue_1.5.1 reshape2_1.4.4 ## [37] dplyr_1.0.7 posterior_1.1.0 V8_3.6.0 ## [40] jquerylib_0.1.4 vctrs_0.3.8 nlme_3.1-152 ## [43] blogdown_1.5 crosstalk_1.2.0 tensorA_0.36.2 ## [46] xfun_0.28 stringr_1.4.0 ps_1.6.0 ## [49] lme4_1.1-27.1 mime_0.12 miniUI_0.1.1.1 ## [52] lifecycle_1.0.1 renv_0.14.0 gtools_3.9.2 ## [55] MASS_7.3-54 zoo_1.8-9 scales_1.1.1 ## [58] colourpicker_1.1.1 Brobdingnag_1.2-6 promises_1.2.0.1 ## [61] inline_0.3.19 shinystan_2.5.0 yaml_2.2.1 ## [64] curl_4.3.2 gridExtra_2.3 loo_2.4.1 ## [67] sass_0.4.0 stringi_1.7.6 highr_0.9 ## [70] dygraphs_1.1.1.6 checkmate_2.0.0 boot_1.3-28 ## [73] pkgbuild_1.3.0 shape_1.4.6 rlang_0.4.12 ## [76] pkgconfig_2.0.3 matrixStats_0.61.0 distributional_0.2.2 ## [79] evaluate_0.14 lattice_0.20-44 purrr_0.3.4 ## [82] rstantools_2.1.1 htmlwidgets_1.5.4 labeling_0.4.2 ## [85] processx_3.5.2 tidyselect_1.1.1 plyr_1.8.6 ## [88] bookdown_0.24 R6_2.5.1 generics_0.1.1 ## [91] DBI_1.1.1 pillar_1.6.4 withr_2.4.3 ## [94] xts_0.12.1 survival_3.2-11 abind_1.4-5 ## [97] tibble_3.1.6 crayon_1.4.2 arrayhelpers_1.1-0 ## [100] utf8_1.2.2 rmarkdown_2.11 grid_4.1.1 ## [103] data.table_1.14.2 callr_3.7.0 threejs_0.3.3 ## [106] xtable_1.8-4 tidyr_1.1.4 httpuv_1.6.3 ## [109] RcppParallel_5.1.4 stats4_4.1.1 munsell_0.5.0 ## [112] bslib_0.3.1 shinyjs_2.0.0  ","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638316800,"objectID":"d972a1012fcea5eb8caa2634a732ede8","permalink":"https://dsba6010-spring2022.netlify.app/resource/install-rethinking/","publishdate":"2021-12-01T00:00:00Z","relpermalink":"/resource/install-rethinking/","section":"resource","summary":"We’ll primarily use Richard McElreath’s rethinking R package. Installation can be a bit tricky so this guide will outline how to install as well as other packages that may be helpful in this class.","tags":null,"title":"Installing rethinking, Stan, and Bayesian Packages","type":"docs"},{"authors":null,"categories":null,"content":"  Chapter 4: Geocentric models\nLecture Lecture 3   Slides  Comprehension questions TBD\nFeedback Go to this form and answer these three questions:\nWhat was the muddiest thing from class this lesson? What are you still wondering about? What was the clearest thing from class this lesson? What was the most exciting thing you learned?  I’ll compile the questions and review for class.\n","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638316800,"objectID":"28a635378a481da28d88cfc6fa1f9db1","permalink":"https://dsba6010-spring2022.netlify.app/content/03-linear-models/","publishdate":"2021-12-01T00:00:00Z","relpermalink":"/content/03-linear-models/","section":"content","summary":"  Chapter 4: Geocentric models\n","tags":["Bayesian methods"],"title":"Class 3 - Linear Models","type":"docs"},{"authors":null,"categories":null,"content":"   Learning R I highly recommend subscribing to the R Weekly newsletter. This e-mail is sent every Monday and is full of helpful tutorials about how to do stuff with R.\nSearching for help with R on Google can sometimes be tricky because the program name is a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.g. “rstats scatterplot”).\nIf you use Twitter, post R-related questions and content with #rstats. The community there is exceptionally generous and helpful. Also check out StackOverflow (a Q\u0026amp;A site with hundreds of thousands of answers to all sorts of programming questions) and RStudio Community (a forum specifically designed for people using RStudio and the tidyverse (i.e. you)).\nThese resources are also really really helpful:\n R for Data Science: A free online book for learning the basics of R and the tidyverse. R and RStudio cheat sheets: A large collection of simple cheat sheets for RStudio, ggplot2, and other R-related things. Stat 545: Dr. Jenny Bryan at RStudio has an entire introductory course in R, visualization, and data analysis online. STA 112FS: Data Science: Dr. Mine Çetinkaya-Rundel at the University of Edinburgh / Duke University has an entire introductory course in R, visualization, and data science online. CSE 631: Principles \u0026amp; Practice of Data Visualization: Yet another introductory course for R and ggplot2 by Dr. Alison Presmanes Hill at RStudio.   R in the wild A popular (and increasingly standard) way for sharing your analyses and visualizations is to post an annotated explanation of your process somewhere online. RStudio allows you to publish knitted HTML files directly to RPubs, but you can also post your output to a blog or other type of website.1 Reading these kinds of posts is one of the best ways to learn R, since they walk you through each step of the process and show the code and output.\nHere are some of the best examples I’ve come across:\n Text analysis of Trump’s tweets confirms he writes only the (angrier) Android half (with a follow-up) Bob Ross - Joy of Painting Bechdel analysis using the tidyverse: There are also a bunch of other examples using data from FiveThirtyEight. Sexism on the Silver Screen: Exploring film’s gender divide Comparison of Quentin Tarantino Movies by Box Office and the Bechdel Test Who came to vote in Utah’s caucuses? Health care indicators in Utah counties Song lyrics across the United States A decade (ish) of listening to Sigur Rós When is Tom peeping these days?: There are a also bunch of final projects from other R and data visualization classes here and here. Mapping Fall Foliage General (Attys) Distributions Disproving Approval    If you want to be really fancy, you can use blogdown, which makes a complete website with R Markdown files. That’s actually how this site is built (see the source code). You can build your own site with this tutorial.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0f6270d48011ac62645a8455a86a24bf","permalink":"https://dsba6010-spring2022.netlify.app/resource/r/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/r/","section":"resource","summary":"Learning R I highly recommend subscribing to the R Weekly newsletter. This e-mail is sent every Monday and is full of helpful tutorials about how to do stuff with R.","tags":null,"title":"R","type":"docs"},{"authors":null,"categories":null,"content":"   R style conventions R is fairly forgiving about how you type code (unlike other languages like Python, where miscounting spaces can ruin your code!). All of these things will do exactly the same thing:\nmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty\u0026gt;10, class==\u0026quot;compact\u0026quot;) filter(mpg,cty\u0026gt;10,class==\u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) filter ( mpg,cty\u0026gt;10, class==\u0026quot;compact\u0026quot; ) But you’ll notice that only a few of those iterations (the first three) are easily readable.\nTo help improve readability and make it easier to share code with others, there’s an unofficial style guide for writing R code. It’s fairly short and just has lots of examples of good and bad ways of writing code (naming variables, dealing with long lines, using proper indentation levels, etc.)—you should glance through it some time.\nRStudio has a built-in way of cleaning up your code. Select some code, press ctrl + i (on Windows) or ⌘ + i (on macOS), and R will reformat the code for you. It’s not always perfect, but it’s really helpful for getting indentation right without having to manually hit space a billion times.\n Main style things to pay attention to for this class  Important note: I won’t ever grade you on any of this! If you submit something like filter(mpg,cty\u0026gt;10,class==\"compact\"), I might recommend adding spaces, but it won’t affect your grade or points or anything.\n Spacing  See the “Spacing” section in the tidyverse style guide.\n Put spaces after commas (like in regular English):\n# Good filter(mpg, cty \u0026gt; 10) # Bad filter(mpg , cty \u0026gt; 10) filter(mpg ,cty \u0026gt; 10) filter(mpg,cty \u0026gt; 10) Put spaces around operators like +, -, \u0026gt;, =, etc.:\n# Good filter(mpg, cty \u0026gt; 10) # Bad filter(mpg, cty\u0026gt;10) filter(mpg, cty\u0026gt; 10) filter(mpg, cty \u0026gt;10) Don’t put spaces around parentheses that are parts of functions:\n# Good filter(mpg, cty \u0026gt; 10) # Bad filter (mpg, cty \u0026gt; 10) filter ( mpg, cty \u0026gt; 10) filter( mpg, cty \u0026gt; 10 )  Long lines  See the “Long lines” section in the tidyverse style guide.\n It’s generally good practice to not have really long lines of code. A good suggestion is to keep lines at a maximum of 80 characters. Instead of counting characters by hand (ew), in RStudio go to “Tools” \u0026gt; “Global Options” \u0026gt; “Code” \u0026gt; “Display” and check the box for “Show margin”. You should now see a really thin line indicating 80 characters. Again, you can go beyond this—that’s fine. It’s just good practice to avoid going too far past it.\nYou can add line breaks inside longer lines of code. Line breaks should come after commas, and things like function arguments should align within the function:\n# Good filter(mpg, cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) # Good filter(mpg, cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) # Good filter(mpg, cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) # Bad filter(mpg, cty \u0026gt; 10, class %in% c(\u0026quot;compact\u0026quot;, \u0026quot;pickup\u0026quot;, \u0026quot;midsize\u0026quot;, \u0026quot;subcompact\u0026quot;, \u0026quot;suv\u0026quot;, \u0026quot;2seater\u0026quot;, \u0026quot;minivan\u0026quot;)) # Good filter(mpg, cty \u0026gt; 10, class %in% c(\u0026quot;compact\u0026quot;, \u0026quot;pickup\u0026quot;, \u0026quot;midsize\u0026quot;, \u0026quot;subcompact\u0026quot;, \u0026quot;suv\u0026quot;, \u0026quot;2seater\u0026quot;, \u0026quot;minivan\u0026quot;))  Pipes (%\u0026gt;%) and ggplot layers (+) Put each layer of a ggplot plot on separate lines, with the + at the end of the line, indented with two spaces:\n# Good ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() # Bad ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() # Super bad ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() # Super bad and won\u0026#39;t even work ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() Put each step in a dplyr pipeline on separate lines, with the %\u0026gt;% at the end of the line, indented with two spaces:\n# Good mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy)) # Bad mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy)) # Super bad mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy)) # Super bad and won\u0026#39;t even work mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy))  Comments  See the “Comments” section in the tidyverse style guide.\n Comments should start with a comment symbol and a single space: #\n# Good #Bad #Bad If the comment is really short (and won’t cause you to go over 80 characters in the line), you can include it in the same line as the code, separated by at least two spaces (it works with one space, but using a couple can enhance readability):\nmpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% # Only rows where cty is 10 + group_by(class) %\u0026gt;% # Divide into class groups summarize(avg_hwy = mean(hwy)) # Find the average hwy in each group You can add extra spaces to get inline comments to align, if you want:\nmpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% # Only rows where cty is 10 + group_by(class) %\u0026gt;% # Divide into class groups summarize(avg_hwy = mean(hwy)) # Find the average hwy in each group If the comment is really long, you can break it into multiple lines. RStudio can do this for you if you go to “Code” \u0026gt; “Reflow comment”\n# Good # Happy families are all alike; every unhappy family is unhappy in its own way. # Everything was in confusion in the Oblonskys’ house. The wife had discovered # that the husband was carrying on an intrigue with a French girl, who had been # a governess in their family, and she had announced to her husband that she # could not go on living in the same house with him. This position of affairs # had now lasted three days, and not only the husband and wife themselves, but # all the members of their family and household, were painfully conscious of it. # Bad # Happy families are all alike; every unhappy family is unhappy in its own way. Everything was in confusion in the Oblonskys’ house. The wife had discovered that the husband was carrying on an intrigue with a French girl, who had been a governess in their family, and she had announced to her husband that she could not go on living in the same house with him. This position of affairs had now lasted three days, and not only the husband and wife themselves, but all the members of their family and household, were painfully conscious of it. Though, if you’re dealing with comments that are that long, consider putting the text in R Markdown instead and having it be actual prose.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f4734e734c67442efdc8d228e91ad766","permalink":"https://dsba6010-spring2022.netlify.app/resource/style/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/style/","section":"resource","summary":"R style conventions R is fairly forgiving about how you type code (unlike other languages like Python, where miscounting spaces can ruin your code!). All of these things will do exactly the same thing:","tags":null,"title":"R style suggestions","type":"docs"},{"authors":null,"categories":null,"content":"  Chapter 4: Geocentric models\nWiggle, wiggle, wiggle\nJust a little bit (Spline!)\n[Trying to show how flexible the spline is with independent samples from prior (top) and posterior (bottom)] pic.twitter.com/Q4JEPwCXuc\n\u0026mdash; Richard McElreath 🍜 (@rlmcelreath) November 14, 2021  Lecture Lecture 4    Comprehension questions TBD\nFeedback Go to this form and answer these three questions:\nWhat was the muddiest thing from class this lesson? What are you still wondering about? What was the clearest thing from class this lesson? What was the most exciting thing you learned?  I’ll compile the questions and review for class.\n","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638316800,"objectID":"a0a50e2045be19e24933558510dd18ac","permalink":"https://dsba6010-spring2022.netlify.app/content/04-linear-models-2/","publishdate":"2021-12-01T00:00:00Z","relpermalink":"/content/04-linear-models-2/","section":"content","summary":"  Chapter 4: Geocentric models\n","tags":["Bayesian methods"],"title":"Class 4 - Linear Models","type":"docs"},{"authors":null,"categories":null,"content":"Individual presentations on Bayesian applications\n","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638316800,"objectID":"abb2c4686400149427db1a5d58aa0f46","permalink":"https://dsba6010-spring2022.netlify.app/content/05-bayesian-presentations/","publishdate":"2021-12-01T00:00:00Z","relpermalink":"/content/05-bayesian-presentations/","section":"content","summary":"Individual presentations on Bayesian applications","tags":null,"title":"Class 5 - Class presentations","type":"docs"},{"authors":null,"categories":null,"content":"   Because RStudio projects typically consist of multiple files (R scripts, datasets, graphical output, etc.) the easiest way to distribute them to you for examples, assignments, and projects is to combine all the different files in to a single compressed collection called a zip file. When you unzip a zipped file, your operating system extracts all the files that are contained inside into a new folder on your computer.\nUnzipping files on macOS is trivial, but unzipping files on Windows can mess you up if you don’t pay careful attention. Here’s a helpful guide to unzipping files on both macOS and Windows.\nUnzipping files on macOS Double click on the downloaded .zip file. macOS will automatically create a new folder with the same name as the .zip file, and all the file’s contents will be inside. Double click on the RStudio Project file (.Rproj) to get started.\n Unzipping files on Windows tl;dr: Right click on the .zip file, select “Extract All…”, and work with the resulting unzipped folder.\nUnlike macOS, Windows does not automatically unzip things for you. If you double click on the .zip file, Windows will show you what’s inside, but it will do so without actually extracting anything. This can be is incredibly confusing! Here’s what it looks like—the only clues that this folder is really a .zip file are that there’s a “Compressed Folder Tools” tab at the top, and there’s a “Ratio” column that shows how much each file is compressed.\nIt is very tempting to try to open files from this view. However, if you do, things will break and you won’t be able to correctly work with any of the files in the zipped folder. If you open the R Project file, for instance, RStudio will point to a bizarre working directory buried deep in some temporary folder:\nYou most likely won’t be able to open any data files or save anything, which will be frustrating.\nInstead, you need to right click on the .zip file and select “Extract All…”:\nThen choose where you want to unzip all the files and click on “Extract”\nYou should then finally have a real folder with all the contents of the zipped file. Open the R Project file and RStudio will point to the correct working directory and everything will work.\n ","date":1588723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588723200,"objectID":"c14c352fd4c4ab8c12a3cd60b30b9d8c","permalink":"https://dsba6010-spring2022.netlify.app/resource/unzipping/","publishdate":"2020-05-06T00:00:00Z","relpermalink":"/resource/unzipping/","section":"resource","summary":"Because RStudio projects typically consist of multiple files (R scripts, datasets, graphical output, etc.) the easiest way to distribute them to you for examples, assignments, and projects is to combine all the different files in to a single compressed collection called a zip file.","tags":null,"title":"Unzipping files","type":"docs"},{"authors":null,"categories":null,"content":"  Chapter 5: The many variables \u0026amp; the spurious waffles\n  Chapter 6: The haunted DAG \u0026amp; the causal terror\nLectures Lecture 5    Lecture 6    Comprehension questions TBD\nFeedback Go to this form and answer these three questions:\nWhat was the muddiest thing from class this lesson? What are you still wondering about? What was the clearest thing from class this lesson? What was the most exciting thing you learned?  I’ll compile the questions and review for class.\n","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638316800,"objectID":"09f5985eb153c476df3d5f36a0da03f2","permalink":"https://dsba6010-spring2022.netlify.app/content/06-spurious-correlations/","publishdate":"2021-12-01T00:00:00Z","relpermalink":"/content/06-spurious-correlations/","section":"content","summary":"  Chapter 5: The many variables \u0026amp; the spurious waffles\n  Chapter 6: The haunted DAG \u0026amp; the causal terror\n","tags":["Causal inference"],"title":"Class 6 - Spurious correlations","type":"docs"},{"authors":null,"categories":null,"content":"    In class session 2 (see this from the FAQ slides) we talked briefly about the difference between frequentist statistics, where you test for the probability of your data given a null hypothesis, or \\(P(\\text{data} \\mid H_0)\\), and Bayesian statistics, where you test for the probability of your hypothesis given your data, or \\(P(H \\mid \\text{data})\\).\nThis difference is important. In the world of frequentism, which is what pretty much all statistics classes use (including this one!), you have to compare your findings to a hypothetical null world and you have to talk about rejecting null hypotheses. In the Bayes world, though, you get to talk about the probability that your hypothesis is correct rather than the probability of seeing a value in a null world. So much more convenient and easy to interpret!\nBayesian statistics, though, requires a lot of computational power and a different way of thinking about statistics and numbers in general. And very few classes teach it. Including this one! I use Bayesian stats all the time in my own research (see this or this, for instance), but don’t teach it (yet!) because nobody else really teaches it and frequentist statistics still rule the policy world, so you need to know it.\nResources But you can learn it on your own. Because very few stats classes actually teach Bayesian statistics, tons of people who use it are self-taught (like me!), in part because there are a ton of resources online for learning this stuff. Here are some of the best I’ve found:\n This new Bayes Rules book is designed to be an introductory textbook for a stats class teaching Bayesian stuff. It’s really accessible and good (and free!). If I ever get to teach an intro stats class with Bayesian stats, I’ll use this. This post from 2016 is a great short introduction and is what made me start using Bayesian methods. The brms package makes it incredibly easy to do Bayesian stuff, and the syntax is basically the same as lm() This post shows how to do one simple task (a difference-in-means test) with regular old frequentist methods, bootstrapping, and with Bayesian stats both with brms and raw Stan code This short post gives a helpful overview of the intuition behind Bayesianism The super canonical everyone-has-this-book book is Statistical Rethinking by Richard McElreath. At that page he also has an entire set of accompanying lectures on YouTube. He doesn’t use brms or ggplot, but someone has translated all his models to tidyverse-based brms code here The Theory That Would Not Die is a fun little general introduction to the history of Bayesianism and why it kind of disappeared in the 20th century and was replaced by frequentism and p-values and null hypothesis testing   Super short example In practice, the R code for Bayesian models should be very familiar. For instance, here’s a regular old frequentist OLS model:\nlibrary(tidyverse) library(broom) model_ols \u0026lt;- lm(hwy ~ displ + drv, data = mpg) tidy(model_ols, conf.int = TRUE) ## # A tibble: 4 × 7 ## term estimate std.error statistic p.value conf.low conf.high ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 30.8 0.924 33.4 4.21e-90 29.0 32.6 ## 2 displ -2.91 0.218 -13.4 1.73e-30 -3.34 -2.48 ## 3 drvf 4.79 0.530 9.05 6.40e-17 3.75 5.83 ## 4 drvr 5.26 0.734 7.17 1.03e-11 3.81 6.70 Here’s that same model using the brms package, with default priors. Note how the code is basically the same:\nlibrary(tidyverse) library(brms) # For Bayesian regression with brm() library(broom.mixed) # For tidy() and glance() with brms-based models library(tidybayes) # For extracting posterior draws # This will take a few seconds to run model_bayes \u0026lt;- brm(hwy ~ displ + drv, data = mpg) tidy(model_bayes) ## # A tibble: 5 × 8 ## effect component group term estimate std.error conf.low conf.high ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 fixed cond \u0026lt;NA\u0026gt; (Intercept) 30.8 0.922 29.0 32.6 ## 2 fixed disp \u0026lt;NA\u0026gt; displ -2.91 0.219 -3.32 -2.48 ## 3 fixed cond \u0026lt;NA\u0026gt; drvf 4.80 0.525 3.76 5.83 ## 4 fixed cond \u0026lt;NA\u0026gt; drvr 5.25 0.754 3.79 6.75 ## 5 ran_pars cond Residual sd__Observation 3.10 0.149 2.81 3.41 In Bayes land, you get a distribution of plausible values given the data (or what is called the “posterior distribution”), and you can visualize this posterior distribution:\n# Make a long dataset of the draws for these three coefficients posterior_draws \u0026lt;- model_bayes %\u0026gt;% gather_draws(c(b_displ, b_drv, b_drvf, b_drvr)) # Plot this thing ggplot(posterior_draws, aes(x = .value, y = fct_rev(.variable), fill = .variable)) + geom_vline(xintercept = 0) + stat_halfeye(.width = c(0.8, 0.95), alpha = 0.8, point_interval = \u0026quot;median_hdi\u0026quot;) + guides(fill = \u0026quot;none\u0026quot;) + labs(x = \u0026quot;Coefficient\u0026quot;, y = \u0026quot;Variable\u0026quot;, caption = \u0026quot;80% and 95% credible intervals shown in black\u0026quot;) Those are all the plausible values for these coefficients, given the data that we’ve fed the model, and the black bars at the bottom show the 80% and 95% credible intervals (or the range of values that 80/95% of the posterior covers). With this, there’s a 95% chance that the coefficient for displacement is between −3.35 and −2.48. Neat!\n Confidence intervals vs. credible intervals In session 6, we talked about frequentist confidence intervals and Bayesian credible (or posterior) intervals, since I had you read Guido Imbens’s essay on p-values, where his conclusion is that:\n It would be preferable if reporting standards emphasized confidence intervals or standard errors, and, even better, Bayesian posterior intervals.\n Imbens wants us to use Bayesian posterior intervals (or credible intervals), but how do we do that?\nFrequentist confidence intervals In frequentist statistics (i.e. all the statistics you’ve been exposed to in this class and all previous classes), your whole goal is to estimate and infer something about a population using a sample. This “something” is a true (but unknown) thing called a population parameter. It is a single fixed value that exists out in the world, and it’s the main thing you’re interested in discovering. Here are a bunch of different population parameters:\n Average treatment effect of a program Proportion of left-handed students at GSU Median rent of apartments in NYC Proportion of red M\u0026amp;Ms produced in a factory  In frequentist statistics, we take a sample from the population, calculate the parameter (i.e. mean, median, proportion, whatever) in the sample, and then check to see how good of a guess it might be for the whole population. To do that, we can look at a confidence interval. Think of a confidence interval as a net—it’s a range of possible values for the population parameters, and we can be X% confident (typically 95%) that the net is picking up the population parameter. Another way to think about it is to imagine taking more samples. If you take 100 samples, at least 95 of them would have the true population parameter in their 95% confidence intervals. Frequentist statistics assumes that the unknown population parameter is fixed and singular, but that the data can vary—you can repeat an experiment over and over again, or take repeated samples from a population in order to be more certain about the estimate of the parameter (and shrink the net of the confidence interval).\nImportantly, when talking about confidence intervals, you cannot really say anything about the estimate of the parameter itself. Confidence intervals are all about the net, or the range itself. You can legally say this:\n We are 95% confident that this confidence interval captures the true population parameter.\n You cannot say this:\n There’s a 95% chance that the population parameter is X. or There’s a 95% chance that the true value falls in this range.\n Confidence intervals tell you about the range, or the net. That’s all.\nHere’s an example with some data from The Effect on restaurant inspections in Alaska. We want to know if weekend inspections are more lenient that ones conducted during the work week.\nlibrary(tidyverse) library(broom) library(gghalves) inspections \u0026lt;- read_csv(\u0026quot;https://vincentarelbundock.github.io/Rdatasets/csv/causaldata/restaurant_inspections.csv\u0026quot;) First we should look at the data to see if there are any obvious patterns. Let’s look at scores separated by weekend status. We’ll use the neat gghalves package to plot both the raw points and a density plot. The orange points show the average value:\nggplot(inspections, aes(x = Weekend, y = inspection_score)) + geom_half_point(side = \u0026quot;l\u0026quot;, alpha = 0.2, size = 0.5, transformation = position_jitter(height = 0)) + geom_half_violin(side = \u0026quot;r\u0026quot;) + stat_summary(fun.data = \u0026quot;mean_se\u0026quot;, fun.args = list(mult = 1.96), color = \u0026quot;orange\u0026quot;) It looks like weekend inspections are far more rare than weekday ones, and no weekend inspections every give scores lower than 80. It also looks like the average weekend score is slightly higher than the average weekday score. Let’s figure out how much of a difference there is.\nBut first, we’ll use the language of inference and sampling. Our population parameter (we’ll call it the Greek letter theta, or \\(\\theta\\)) is some single true fixed number that exists out in the world—weekend restaurant inspections in Alaska have a \\(\\theta\\) higher average score than weekday inspections. We want to find out what that \\(\\theta\\) is, so we’ll look at some confidence intervals.\nWe can look at a basic difference in means based on weekend status:\nmodel_naive \u0026lt;- lm(inspection_score ~ Weekend, data = inspections) tidy(model_naive, conf.int = TRUE) ## # A tibble: 2 × 7 ## term estimate std.error statistic p.value conf.low conf.high ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 93.6 0.0381 2458. 0 93.6 93.7 ## 2 WeekendTRUE 2.10 0.433 4.84 0.00000131 1.25 2.95 Here, weekend scores are 2.1 points higher than weekday scores, on average (that’s our estimate, or \\(\\hat{\\theta}\\). We have a confidence interval of 1.2–2.9. We cannot say that we’re 95% confident that the weekend score boost (or \\(\\theta\\)) is between 1.2 and 2.9. What we can say is that we’re 95% confident that the range 1.2–2.9 captures the true population parameter \\(\\theta\\). If we took a bunch of different samples of inspection scores and calculated the average weekend vs. weekday score in each of those samples, 95% of those confidence intervals should capture the true \\(\\theta\\). Importantly, we still have no idea what the actual \\(\\theta\\) is, but we’re pretty sure that our confidence interval net has captured it.\nThis estimate is probably wrong, since there are other factors that confound the weekend → score relationship. Maybe the health department only conducts weekend inspections in places with lots of branches, or maybe they did more weekend inspections in certain years. We can adjust/control for these in the model:\nmodel_adjusted \u0026lt;- lm(inspection_score ~ Weekend + NumberofLocations + Year, data = inspections) tidy(model_adjusted, conf.int = TRUE) ## # A tibble: 4 × 7 ## term estimate std.error statistic p.value conf.low conf.high ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 225. 12.4 18.1 7.88e-73 200. 249. ## 2 WeekendTRUE 1.43 0.419 3.42 6.25e- 4 0.611 2.25 ## 3 NumberofLocations -0.0191 0.000436 -43.9 0 -0.0200 -0.0183 ## 4 Year -0.0646 0.00617 -10.5 1.45e-25 -0.0767 -0.0525 Our weekend estimate shrunk a little and is now 1.43, with a confidence interval of 0.6–2.3. Again, think of this as a net—we’re 95% sure that the true \\(\\theta\\) is in that net somewhere. \\(\\theta\\) could be 0.7, it could be 1.4, it could be 2.2—who knows. All we know is that our net most likely picked it up.\nFor fun, let’s plot both these weekend estimates and their confidence intervals:\n# Save just the weekend coefficient from both models freq_results_naive \u0026lt;- tidy(model_naive, conf.int = TRUE) %\u0026gt;% mutate(model = \u0026quot;Naive model\u0026quot;) %\u0026gt;% filter(term == \u0026quot;WeekendTRUE\u0026quot;) freq_results_full \u0026lt;- tidy(model_adjusted, conf.int = TRUE) %\u0026gt;% mutate(model = \u0026quot;Full model\u0026quot;) %\u0026gt;% filter(term == \u0026quot;WeekendTRUE\u0026quot;) # Put these coefficients in a single dataset and plot them freq_results \u0026lt;- bind_rows(freq_results_naive, freq_results_full) %\u0026gt;% # Make sure the model name follows the order it appears in the data instead of # alphabetical order mutate(model = fct_inorder(model)) ggplot(freq_results, aes(x = estimate, y = model, color = model)) + geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) + guides(color = \u0026quot;none\u0026quot;)  Bayesian credible intervals Remember, with frequentist statistics, \\(\\theta\\) is fixed and singular and we’re hoping to pick it up with our confidence interval nets. The data we collect is variable—we can hypothetically take more and more samples and calculate a bunch of confidence intervals and become more certain about where \\(\\theta\\) might be. We can only interpret confidence intervals as ranges: “There’s a 95% probability that the range contains the true value \\(\\theta\\)”. We can’t say anything about the estimate of \\(\\theta\\) itself. We’ve calculated the probability of the range, not the probability of the actual value.\nBayesian analysis, however, does let us talk about the probability of the actual value. Under Bayesianism, the data you’re working with is fixed (i.e. you collected it once and it’s all you have—you can’t go out and collect infinite additional samples), and the population parameter \\(\\theta\\) varies and has uncertainty about it (i.e. instead of imagining some single number uncapturable that’s the average difference in weekend vs. weekday scores, \\(\\theta\\) has some range around it).\nThis difference is apparent in the formulas for testing hypotheses under each of these approaches:\n\\[\\underbrace{P(\\text{Data} \\mid \\theta)}_{\\substack{\\textbf{Frequentism} \\\\ \\text{Probability of seeing the} \\\\ \\text{data given that } \\theta \\text{ exists}}} \\qquad \\underbrace{P(\\theta \\mid \\text{Data})}_{\\substack{\\textbf{Bayesianism} \\\\ \\text{Probability of } \\theta \\\\ \\text{given the current data}}}\\]\nBayes’ theorem has a nice formula (with neat intuition, like in this video):\n\\[ \\underbrace{P(\\theta \\mid \\text{Data})}_{\\text{Posterior}} = \\frac{\\overbrace{P(\\theta)}^{\\text{Prior}} \\times \\overbrace{P(\\text{Data} \\mid \\theta)}^{\\text{Likelihood}}}{P(\\text{Data})} \\]\nPut (hopefully!) simply, combine the observed likelihood of the data \\(P(\\text{Data} \\mid \\theta)\\) (that’s basically frequentism!) with prior knowledge about the distribution of \\(\\theta\\) and you’ll get a posterior estimate of \\(\\theta\\).\nActually calculating this with real data, though, can be tricky and computationally intensive—often there’s no formal mathematical way to figure out the actual equation. So instead, we can use computers to simulate thousands of guesses and then look at the distribution of those guesses (just like we did with the Zilch simulation in class). One modern method for doing this is called Monte Carlo Markov Chain (MCMC) simulation, which is what most R-based tools for Bayesian stats use nowadays.\nLet’s look at restaurant inspection scores on the weekend Bayesianly. Here, we’re still interested in our population parameter \\(\\theta\\), or the average weekend score boost. Only now, we’re not assuming that \\(\\theta\\) is some single fixed value out in the world that we’re trying to capture with confidence intervals—we’ll use the data that we have to estimate the variation in \\(\\theta\\). The easiest way to do Bayesian analysis with R is with the brms package, which uses the familiar formula syntax you’ve been using with lm(). The syntax is super similar, just with a few additional arguments:\nMCMC things\nArguments like chains, iter, and cores deal with the simulation. chains defines how many parallel simulations should happen, iter controls how many iterations should happen with in each chain, and cores spreads those chains across the CPUs in your computer (i.e. if you have a 4-core computer, you can run 4 chains all at the same time; run parallel::detectCores() in your R console to see how many CPU cores you have). seed makes it so that the random simulation results are reproducible (see here for more on seeds).\nPriors\nThese define your prior beliefs about the parameters (i.e. \\(\\theta\\)) in the model. If you think that the restaurant weekend inspection boost is probably positive, but could possibly be negative, or maybe zero, you can feed that belief into the model. For instance, if you’re fairly confident (based on experiences in other states maybe) that weekend scores really are higher, you can provide an informative prior that says that \\(\\theta\\) is most likely 1.5 points ± a little variation, following a normal distribution. Or, if you have no idea what it could be—maybe it’s super high like 10, maybe it’s negative like −5, or maybe it’s 0 and there’s no weekend boost—you can provide a vague prior that says that \\(\\theta\\) is 0 points ± a ton of variation.\nlibrary(patchwork) # For combining ggplot plots plot_informative \u0026lt;- ggplot() + stat_function(fun = dnorm, args = list(mean = 1.5, sd = 0.5), geom = \u0026quot;area\u0026quot;, fill = \u0026quot;grey80\u0026quot;, color = \u0026quot;black\u0026quot;) + xlim(-1, 4) + labs(title = \u0026quot;Informative prior\u0026quot;, subtitle = \u0026quot;Normal(mean = 1.5, sd = 0.5)\u0026quot;, caption = \u0026quot;We\u0026#39;re pretty sure θ is around 1.5\u0026quot;) plot_vague \u0026lt;- ggplot() + stat_function(fun = dnorm, args = list(mean = 0, sd = 10), geom = \u0026quot;area\u0026quot;, fill = \u0026quot;grey80\u0026quot;, color = \u0026quot;black\u0026quot;) + xlim(-35, 35) + labs(title = \u0026quot;Vague prior\u0026quot;, subtitle = \u0026quot;Normal(mean = 0, sd = 10)\u0026quot;, caption = \u0026quot;Surely θ is in there *somewhere*\u0026quot;) plot_informative | plot_vague For the sake of this example, we’ll use a vague prior.\nHere’s how to officially do Bayesian analysis with brms and incorporate prior information about \\(\\theta\\). Again, the syntax is super similar to lm(), just with some extra bits about the prior and the MCMC settings:\nlibrary(brms) # For Bayesian regression with brm() library(broom.mixed) # For tidy() and glance() with brms-based models library(tidybayes) # For extracting posterior draws library(ggdist) # For making pretty posterior plots # bf() stands for \u0026quot;bayes formula\u0026quot;; you technically don\u0026#39;t need to use it, but it # makes life easier for more complex models, so it\u0026#39;s good practice even when # using a simple formula like the one here # # This will take a little bit of time to run. Here\u0026#39;s what it\u0026#39;s actually doing: # # 1. Translate this R code to Stan (a specific language for doing Bayesian stuff with MCMC) # 2. Compile the Stan code to faster-running C++ code # 3. Actually do the MCMC sampling # Set the prior for the weekend coefficient # Use get_priors() to see all the other default priors priors \u0026lt;- c( prior(normal(0, 10), class = \u0026quot;b\u0026quot;, coef = \u0026quot;WeekendTRUE\u0026quot;) ) # Run the model! model_bayes \u0026lt;- brm(bf(inspection_score ~ Weekend + NumberofLocations + Year), data = inspections, prior = priors, chains = 4, iter = 2000, cores = 4, seed = 1234) ## Compiling Stan program... ## Start sampling Phew. That took a while to run, but it ran! Now we can check the results:\ntidy(model_bayes, conf.int = TRUE) ## # A tibble: 5 × 8 ## effect component group term estimate std.error conf.low conf.high ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 fixed cond \u0026lt;NA\u0026gt; (Intercept) 224. 12.5 200. 249. ## 2 fixed cond \u0026lt;NA\u0026gt; WeekendTRUE 1.44 0.420 0.613 2.25 ## 3 fixed cond \u0026lt;NA\u0026gt; NumberofLocations -0.0191 0.000428 -0.0200 -0.0183 ## 4 fixed cond \u0026lt;NA\u0026gt; Year -0.0644 0.00624 -0.0765 -0.0522 ## 5 ran_pars cond Residual sd__Observation 6.04 0.0252 5.99 6.09 Our estimate for the weekend boost, or \\(\\hat{\\theta}\\), is 1.44, which is basically the same as the frequentist estimate we found before. We have an interval too, but it’s not a confidence interval—it’s a credible interval. Instead of telling us about the range of the confidence interval net, this credible interval tells us the probability that \\(\\hat{\\theta}\\) falls in that range. It’s essentially the probability of the actual value, not the probability of the range. Based on this, there’s a 95% chance that—given the data we have—the weekend score boost (\\(\\hat{\\theta}\\)) is between 0.61 and 2.25.\nWe can visualize this posterior distribution to see more information than we could with our frequentist estimate. Remember, our simulation estimated thousands of possible coefficients for WeekendTRUE, and each of them are equally likely. The value that we see in tidy() is the median of all these simulated coefficients, or draws. We can see a few of them here:\nmodel_bayes %\u0026gt;% spread_draws(b_WeekendTRUE) %\u0026gt;% head(10) ## # A tibble: 10 × 4 ## .chain .iteration .draw b_WeekendTRUE ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 1 1 0.923 ## 2 1 2 2 1.30 ## 3 1 3 3 1.11 ## 4 1 4 4 0.917 ## 5 1 5 5 1.54 ## 6 1 6 6 1.33 ## 7 1 7 7 1.21 ## 8 1 8 8 1.39 ## 9 1 9 9 1.79 ## 10 1 10 10 1.09 Sometimes the weekend boost is 1.2, sometimes 1.7, sometimes 1.3, etc. There’s a lot of variation in there. We can plot all these simulated coefficients to see where they mostly cluster:\nweekend_draws \u0026lt;- model_bayes %\u0026gt;% spread_draws(b_WeekendTRUE) ggplot(weekend_draws, aes(x = b_WeekendTRUE)) + stat_halfeye() + labs(caption = \u0026quot;Point shows median value;\\nthick black bar shows 66% credible interval;\\nthin black bar shows 95% credible interval\u0026quot;) The weekend point boost \\(\\hat{\\theta}\\) is mostly clustered around 1–2, and 95% of those draws are between 0.61 and 2.25. We’re thus 95% sure that the actual weekend point boost is between 0.61 and 2.25 with a median of 1.44.\nWe can also look at this distribution a slightly different way by collapsing all those posterior draws into 100 possible values. Each of these dots is equally likely, and the true value of \\(\\theta\\) could be any of them, but again, most are clustered around 1.44:\nggplot(weekend_draws, aes(x = b_WeekendTRUE)) + stat_dotsinterval(quantiles = 100) + labs(caption = \u0026quot;Point shows median value;\\nthick black bar shows 66% credible interval;\\nthin black bar shows 95% credible interval\u0026quot;)   General summary of intervals So, we’ve seen two different philosophies for quantifying uncertainty with confidence intervals and credible intervals. Here’s a general overview of the two approaches and how you can interpret them:\n   Frequentism  Bayesianism      Approach  \\(P(\\text{Data} \\mid \\theta)\\)  \\(P(\\theta \\mid \\text{Data})\\)     \\(\\theta\\) is a fixed single value; data is variable and can be repeatedly sampled  \\(\\theta\\) is variable and has uncertainty; data is fixed (you only have one sample)    How to do it in R  lm(…)  library(brms)\nbrm(…)    Name  Confidence interval  Credible interval (or posterior interval)    Intuition  Probability of the range  Probability of the actual value    Interpretation template  There’s a 95% probability that this range contains the true value of \\(\\theta\\)  There’s a 95% probability that the true value of \\(\\theta\\) falls in this range.     Few people naturally think like this  People do naturally think like this      Two ways of making decisions with posterior distributions In the world of frequentism, we’re interested in whether coefficients are statistically different from 0 in a null world where there’s no effect. We rely on p-values to see the probability of seeing an estimate at least as large as what we’ve calculated in a hypothetical world where that estimate is actually 0. This is a really non-intuitive way of thinking about the world (imaginary null worlds?!), so everyone always misinterprets p-values.\nRemember what you read in Imbens’s article though—in real life, very few people care about whether a coefficient is significantly different from a hypothetical null. Instead, people want to know how certain you are of the estimate and what it means practically. Is it for sure a positive effect, or could it maybe be zero or maybe be negative? Significance stars can’t tell us much about those questions, but posterior Bayesian intervals can.\nProbability of direction One question we can answer with Bayesian results is “How certain are we that this estimate is positive (or negative)?” Are we sure the weekend scores are higher on average, or could they sometimes be negative? Are we sure that the average treatment effect of your program decreases poverty, or could it maybe have a positive effect instead?\nTo figure this out, we can calculate something called the “probability of direction,” or the proportion of posterior draws that are above (or below) some arbitrary number. For instance, what’s the probability that the weekend boost is positive (or greater than 0)?\n# Find the proportion of posterior draws that are bigger than 0 weekend_draws %\u0026gt;% summarize(prop_greater_0 = sum(b_WeekendTRUE \u0026gt; 0) / n()) ## # A tibble: 1 × 1 ## prop_greater_0 ## \u0026lt;dbl\u0026gt; ## 1 1.00 Whoa. 99.9% of the posterior draws for the weekend boost are greater than 0, meaning that there’s a 99.9% chance that the coefficient is positive, given the data we have.\nThe neat thing about the probability of direction is that we can choose whatever value we want as the threshold. Let’s say the state health director wants to know if weekend scores are higher than weekday scores, but she’s fine with just a little boost (weekends are nice! inspectors are happier!). Pretend that she thinks an average difference of 1 or lower isn’t a big concern, but seeing a difference greater than 1 is a signal that weekend inspectors are maybe being too lenient. We can use 1 as our threshold instead:\n# Find the proportion of posterior draws that are bigger than 1 weekend_draws %\u0026gt;% summarize(prop_greater_0 = sum(b_WeekendTRUE \u0026gt; 1) / n()) ## # A tibble: 1 × 1 ## prop_greater_0 ## \u0026lt;dbl\u0026gt; ## 1 0.855 Based on this, 84% of the draws are higher than 1, so there’s an 84% chance that the actual \\(\\theta\\) is greater than 1. Notice how there’s no discussion of significance here—no alpha thresholds, no stars, no null worlds. We just have a probability that \\(\\hat{\\theta}\\) is above 1. We can even visualize it. Everything to the right of that vertical line at 1 is “significant” (but not significant with null worlds and stars).\nggplot(weekend_draws, aes(x = b_WeekendTRUE)) + stat_halfeye(aes(fill_ramp = stat(x \u0026gt; 1)), fill = \u0026quot;red\u0026quot;) + scale_fill_ramp_discrete(from = \u0026quot;darkred\u0026quot;, guide = \u0026quot;none\u0026quot;) + geom_vline(xintercept = 1) + labs(caption = \u0026quot;Point shows median value;\\nthick black bar shows 66% credible interval;\\nthin black bar shows 95% credible interval\u0026quot;) Should the state health director be concerned? Probably. There’s an 84% chance that weekend inspection scores are at least 1 point higher than weekday scores, on average, given the data we have.\n Region of practical equivalence (ROPE) Looking at the probability of direction is helpful if you are concerned whether an effect is positive or negative (i.e. greater or less than 0), but it’s also a little weird to think about because we’re testing if something is greater or less than some specific single number. In our example of the health director, we pretended that she cared whether the average weekend score was 1 point higher, but that’s arbitrary.\nAnother approach is that we can think of a range of \\(\\theta\\) where there’s practically no effect. Think of this as a “dead zone” of sorts. If \\(\\hat{\\theta}\\) is 0, we know there’s no effect. If \\(\\hat{\\theta}\\) is something tiny like 0.2 or -0.3, we probably don’t actually care—that’s a tiny amount and could just be because of measurement error. It’s not anything really actionable. If \\(\\hat{\\theta}\\) is big like 1.3 or -2.4 or whatever, then we have cause to worry, but if the estimate is in the “dead zone” (however we want to define it), then we shouldn’t really care or worry.\nThe official Bayesian term for this “dead zone” is the region of practical equivalence (ROPE). There are lots of ways to determine this dead zone—you can base it on experience with the phenomenon (e.g., if you’re the health director and know a lot about inspection scores, you know what kind of score ranges matter), or you can base it on the data you have (e.g., -0.1 * sd(outcome) to 0.1 * sd(outcome)).\nFor this example, let’s pretend that the health director tells you that any effect between −0.5 and 0.5 doesn’t matter—for her, those kind of values would be the same as 0. Now that we have a dead zone or ROPE, we can calculate the proportion of coefficient draws that fall outside of that ROPE:\n# Find the proportion of posterior draws that are bigger than 0.5 or less than -0.5 weekend_draws %\u0026gt;% summarize(prop_outside_rope = 1 - sum(b_WeekendTRUE \u0026gt;= -0.5 \u0026amp; b_WeekendTRUE \u0026lt;= 0.5) / n()) ## # A tibble: 1 × 1 ## prop_outside_rope ## \u0026lt;dbl\u0026gt; ## 1 0.986 ggplot(weekend_draws, aes(x = b_WeekendTRUE)) + stat_halfeye(aes(fill_ramp = stat(x \u0026gt;= 0.5 | x \u0026lt;= -0.5)), fill = \u0026quot;red\u0026quot;) + scale_fill_ramp_discrete(from = \u0026quot;darkred\u0026quot;, guide = \u0026quot;none\u0026quot;) + annotate(geom = \u0026quot;rect\u0026quot;, xmin = -0.5, xmax = 0.5, ymin = -Inf, ymax = Inf, fill = \u0026quot;purple\u0026quot;, alpha = 0.3) + annotate(geom = \u0026quot;label\u0026quot;, x = 0, y = 0.75, label = \u0026quot;ROPE\\n(dead zone)\u0026quot;) + labs(caption = \u0026quot;Point shows median value;\\nthick black bar shows 66% credible interval;\\nthin black bar shows 95% credible interval\u0026quot;) Given this data, 98% of the posterior distribution of the weekend boost is outside of the ROPE, or dead zone, so we can consider this to be “significant” (again, this is a tricky word because it has nothing to do with null worlds and stars!).\nThere are some debates over what you should check with the ROPE. Some people say that you should look at how much of the 95% credible interval is inside the dead zone; other say you should look at how much of the entire distribution is inside the dead zone. We just did the latter, with the whole distribution. If we want to see how much of the area within the credible interval is inside the dead zone, we can change the code a little to filter those observations out:\n# Extract the 95% confidence interval range weekend_cred_int \u0026lt;- weekend_draws %\u0026gt;% median_hdi() weekend_cred_int$.lower ## [1] 0.66 # Find the proportion of posterior draws that are bigger than 0.5 or less than # -0.5, but only look inside the 95% credible interval weekend_draws %\u0026gt;% # Only look inside the credible interval filter(b_WeekendTRUE \u0026gt;= weekend_cred_int$.lower \u0026amp; b_WeekendTRUE \u0026lt;= weekend_cred_int$.upper) %\u0026gt;% summarize(prop_outside_rope = 1 - sum(b_WeekendTRUE \u0026gt;= -0.5 \u0026amp; b_WeekendTRUE \u0026lt;= 0.5) / n()) ## # A tibble: 1 × 1 ## prop_outside_rope ## \u0026lt;dbl\u0026gt; ## 1 1 If we look only at the 95% credible interval of the posterior, there’s a 0% chance that any of those estimated coefficients are in the dead zone / ROPE. There’s a 100% chance that the credible interval doesn’t touch the ROPE. You can see this visually too—look at the figure above with the purple ROPE. The thin black bar that shows the 95% credible interval doesn’t show up in the purple area.\nWhich approach is better—using full distribution or just using the credible interval? Who knows. That’s up to you.\nFinally, here we decided on the ROPE kind of arbitrarily as −0.5 to 0.5, but there are more systematic ways of doing it. One common and standard suggestion is to use −0.1 and 0.1 times the standard deviation of the outcome variable:\nc(-0.1, 0.1) * sd(inspections$inspection_score) ## [1] -0.63 0.63 Based on this approach, our ROPE/dead zone should be −0.63 to 0.63. Let’s see how that looks:\n# Find the proportion of posterior draws that are bigger than 0.5 or less than # -0.5, but only look inside the 95% credible interval weekend_draws %\u0026gt;% # Only look inside the credible interval filter(b_WeekendTRUE \u0026gt;= weekend_cred_int$.lower \u0026amp; b_WeekendTRUE \u0026lt;= weekend_cred_int$.upper) %\u0026gt;% summarize(prop_outside_rope = 1 - sum(b_WeekendTRUE \u0026gt;= -0.63 \u0026amp; b_WeekendTRUE \u0026lt;= 0.63) / n()) ## # A tibble: 1 × 1 ## prop_outside_rope ## \u0026lt;dbl\u0026gt; ## 1 1 ggplot(weekend_draws, aes(x = b_WeekendTRUE)) + stat_halfeye(aes(fill_ramp = stat(x \u0026gt;= 0.63 | x \u0026lt;= -0.63)), fill = \u0026quot;red\u0026quot;) + scale_fill_ramp_discrete(from = \u0026quot;darkred\u0026quot;, guide = \u0026quot;none\u0026quot;) + annotate(geom = \u0026quot;rect\u0026quot;, xmin = -0.63, xmax = 0.63, ymin = -Inf, ymax = Inf, fill = \u0026quot;purple\u0026quot;, alpha = 0.3) + annotate(geom = \u0026quot;label\u0026quot;, x = 0, y = 0.75, label = \u0026quot;ROPE\\n(dead zone)\u0026quot;) + labs(caption = \u0026quot;Point shows median value;\\nthick black bar shows 66% credible interval;\\nthin black bar shows 95% credible interval\u0026quot;) This changes our results just a tiny bit. 97% of the full posterior distribution and 99.7% of the credible interval falls outside this ROPE. Neat. We can thus safely say that the weekend effect, or our estimate of \\(\\theta\\) is definitely practical and substantial (or “significant” if we want to play with that language).\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6b38c913439f6d426eea404774574981","permalink":"https://dsba6010-spring2022.netlify.app/resource/bayes/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/bayes/","section":"resource","summary":"In class session 2 (see this from the FAQ slides) we talked briefly about the difference between frequentist statistics, where you test for the probability of your data given a null hypothesis, or \\(P(\\text{data} \\mid H_0)\\), and Bayesian statistics, where you test for the probability of your hypothesis given your data, or \\(P(H \\mid \\text{data})\\).","tags":null,"title":"Bayesian statistics resources","type":"docs"},{"authors":null,"categories":null,"content":"  Chapter 7: Ulysses' Compass\nLecture Lecture 7    Lecture 8    Comprehension questions TBD\nFeedback Go to this form and answer these three questions:\nWhat was the muddiest thing from class this lesson? What are you still wondering about? What was the clearest thing from class this lesson? What was the most exciting thing you learned?  I’ll compile the questions and review for class.\n","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638316800,"objectID":"8ccb24c25d363912affb1e52e639f8f3","permalink":"https://dsba6010-spring2022.netlify.app/content/07-model-comparison/","publishdate":"2021-12-01T00:00:00Z","relpermalink":"/content/07-model-comparison/","section":"content","summary":"  Chapter 7: Ulysses' Compass\n","tags":["Causal inference"],"title":"Class 7 - Model Comparison","type":"docs"},{"authors":null,"categories":null,"content":"  Chapter 8: Conditional Manatees\n  Chapter 9: Markov Chain Monte Carlo\nLecture Lecture 9    Lecture 10    ## Comprehension questions TBD\nFeedback Go to this form and answer these three questions:\nWhat was the muddiest thing from class this lesson? What are you still wondering about? What was the clearest thing from class this lesson? What was the most exciting thing you learned?  I’ll compile the questions and review for class.\n","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638316800,"objectID":"ea3397f787151cdd7d41349929c3ecd7","permalink":"https://dsba6010-spring2022.netlify.app/content/08-interactions/","publishdate":"2021-12-01T00:00:00Z","relpermalink":"/content/08-interactions/","section":"content","summary":"  Chapter 8: Conditional Manatees\n  Chapter 9: Markov Chain Monte Carlo\n","tags":["Causal inference"],"title":"Class 8 - Interactions/MCMC","type":"docs"},{"authors":null,"categories":null,"content":"TBD\n","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638316800,"objectID":"6dd61808c936d23ef295cc60ef82a321","permalink":"https://dsba6010-spring2022.netlify.app/content/10-guest-lecture/","publishdate":"2021-12-01T00:00:00Z","relpermalink":"/content/10-guest-lecture/","section":"content","summary":"TBD","tags":["Computation"],"title":"Class 10 - Guest Lecture","type":"docs"},{"authors":null,"categories":null,"content":"  Chapter 12: Monsters and Mixtures\nLecture Lecture 13    Lecture 14    Comprehension questions TBD\nFeedback Go to this form and answer these three questions:\nWhat was the muddiest thing from class this lesson? What are you still wondering about? What was the clearest thing from class this lesson? What was the most exciting thing you learned?  I’ll compile the questions and review for class.\n","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638316800,"objectID":"51161f833f1ba2df48c5163de4803c8b","permalink":"https://dsba6010-spring2022.netlify.app/content/11-monsters-mixtures/","publishdate":"2021-12-01T00:00:00Z","relpermalink":"/content/11-monsters-mixtures/","section":"content","summary":"  Chapter 12: Monsters and Mixtures\n","tags":["Computation"],"title":"Class 11 - GLM / Categorical","type":"docs"},{"authors":null,"categories":null,"content":"  Chapter 12: Monsters and Mixtures\nLecture Lecture 11    Lecture 12    Comprehension questions TBD\nFeedback Go to this form and answer these three questions:\nWhat was the muddiest thing from class this lesson? What are you still wondering about? What was the clearest thing from class this lesson? What was the most exciting thing you learned?  I’ll compile the questions and review for class.\n","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638316800,"objectID":"c2ffa8492b7169e82c25cf1114ff2bc5","permalink":"https://dsba6010-spring2022.netlify.app/content/09-entropy-glm/","publishdate":"2021-12-01T00:00:00Z","relpermalink":"/content/09-entropy-glm/","section":"content","summary":"  Chapter 12: Monsters and Mixtures\n","tags":["Computation"],"title":"Class 11 - Monsters \u0026 Mixtures","type":"docs"},{"authors":null,"categories":null,"content":"  Chapter 13: Models with Memory\nLecture Lecture 15    Lecture 16    Comprehension questions TBD\nFeedback Go to this form and answer these three questions:\nWhat was the muddiest thing from class this lesson? What are you still wondering about? What was the clearest thing from class this lesson? What was the most exciting thing you learned?  I’ll compile the questions and review for class.\n","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638316800,"objectID":"e894b2f99698322a081d7f4ee6cb9e7d","permalink":"https://dsba6010-spring2022.netlify.app/content/12-multilevel/","publishdate":"2021-12-01T00:00:00Z","relpermalink":"/content/12-multilevel/","section":"content","summary":"  Chapter 13: Models with Memory\n","tags":["Computation"],"title":"Class 12 - Multilevel","type":"docs"},{"authors":null,"categories":null,"content":"In class exam.\n","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638316800,"objectID":"3576cf63b0b65bdfc3b008a6dd036469","permalink":"https://dsba6010-spring2022.netlify.app/content/13-exam/","publishdate":"2021-12-01T00:00:00Z","relpermalink":"/content/13-exam/","section":"content","summary":"In class exam.","tags":["Computation"],"title":"Class 13 - Exam","type":"docs"},{"authors":null,"categories":null,"content":"  Chapter 14: Adventures in Covariance\nLecture Lecture 17    Lecture 18    Comprehension questions TBD\nFeedback Go to this form and answer these three questions:\nWhat was the muddiest thing from class this lesson? What are you still wondering about? What was the clearest thing from class this lesson? What was the most exciting thing you learned?  I’ll compile the questions and review for class.\n","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638316800,"objectID":"316d389f8b839dccb383b42e19d17b2c","permalink":"https://dsba6010-spring2022.netlify.app/content/14-covariance/","publishdate":"2021-12-01T00:00:00Z","relpermalink":"/content/14-covariance/","section":"content","summary":"Chapter 14: Adventures in Covariance\nLecture Lecture 17    Lecture 18    Comprehension questions TBD\nFeedback Go to this form and answer these three questions:","tags":["Computation"],"title":"Class 14 - Adventures in Covariance","type":"docs"},{"authors":null,"categories":null,"content":"  Chapter 15: Missing Data and Other Opportunities\nLecture Lecture 19       Comprehension questions TBD\nFeedback Go to this form and answer these three questions:\nWhat was the muddiest thing from class this lesson? What are you still wondering about? What was the clearest thing from class this lesson? What was the most exciting thing you learned?  I’ll compile the questions and review for class.\n","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638316800,"objectID":"d32cfbadcc571aa36f6caec7d7da8af4","permalink":"https://dsba6010-spring2022.netlify.app/content/15-missing-measurement/","publishdate":"2021-12-01T00:00:00Z","relpermalink":"/content/15-missing-measurement/","section":"content","summary":"Chapter 15: Missing Data and Other Opportunities\nLecture Lecture 19       Comprehension questions TBD\nFeedback Go to this form and answer these three questions:","tags":["Computation"],"title":"Class 15 - Missing Data","type":"docs"},{"authors":null,"categories":null,"content":"  15.1 Examples # simulate a pancake and return randomly ordered sides sim_pancake \u0026lt;- function() { pancake \u0026lt;- sample(1:3,1) sides \u0026lt;- matrix(c(1,1,1,0,0,0),2,3)[,pancake] sample(sides) } # sim 10,000 pancakes pancakes \u0026lt;- replicate( 1e4 , sim_pancake() ) up \u0026lt;- pancakes[1,] down \u0026lt;- pancakes[2,] # compute proportion 1/1 (BB) out of all 1/1 and 1/0 num_11_10 \u0026lt;- sum( up==1 ) num_11 \u0026lt;- sum( up==1 \u0026amp; down==1 ) print(num_11/num_11_10) ## [1] 0.65834  15.2 Measurement Error data(WaffleDivorce, package = \u0026quot;rethinking\u0026quot;) d \u0026lt;- WaffleDivorce rm(WaffleDivorce) # points plot( d$Divorce ~ d$MedianAgeMarriage , ylim=c(4,15) , xlab=\u0026quot;Median age marriage\u0026quot; , ylab=\u0026quot;Divorce rate\u0026quot; ) # standard errors for ( i in 1:nrow(d) ) { ci \u0026lt;- d$Divorce[i] + c(-1,1)*d$Divorce.SE[i] x \u0026lt;- d$MedianAgeMarriage[i] lines( c(x,x) , ci ) } dlist \u0026lt;- list( D_obs = standardize( d$Divorce ), D_sd = d$Divorce.SE / sd( d$Divorce ), M = standardize( d$Marriage ), A = standardize( d$MedianAgeMarriage ), N = nrow(d) ) m15.1 \u0026lt;- ulam( alist( D_obs ~ dnorm( D_true , D_sd ), vector[N]:D_true ~ dnorm( mu , sigma ), mu \u0026lt;- a + bA*A + bM*M, a ~ dnorm(0,0.2), bA ~ dnorm(0,0.5), bM ~ dnorm(0,0.5), sigma ~ dexp(1) ) , data=dlist , chains=4 , cores=4, cmdstan=TRUE ) ## R code 15.4 precis( m15.1 , depth=2 ) #plot(m15.1)  ","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638316800,"objectID":"1cf84f25a13d240154f8fcba580cd535","permalink":"https://dsba6010-spring2022.netlify.app/example/15-1-example/","publishdate":"2021-12-01T00:00:00Z","relpermalink":"/example/15-1-example/","section":"example","summary":"15.1 Examples # simulate a pancake and return randomly ordered sides sim_pancake \u0026lt;- function() { pancake \u0026lt;- sample(1:3,1) sides \u0026lt;- matrix(c(1,1,1,0,0,0),2,3)[,pancake] sample(sides) } # sim 10,000 pancakes pancakes \u0026lt;- replicate( 1e4 , sim_pancake() ) up \u0026lt;- pancakes[1,] down \u0026lt;- pancakes[2,] # compute proportion 1/1 (BB) out of all 1/1 and 1/0 num_11_10 \u0026lt;- sum( up==1 ) num_11 \u0026lt;- sum( up==1 \u0026amp; down==1 ) print(num_11/num_11_10) ## [1] 0.","tags":null,"title":"Example: 15-1","type":"example"},{"authors":null,"categories":null,"content":"    Markdown is a special kind of markup language that lets you format text with simple syntax. You can then use a converter program like pandoc to convert Markdown into whatever format you want: HTML, PDF, Word, PowerPoint, etc. (see the full list of output types here)\nDelete this text and start playing with Markdown here!   Basic Markdown formatting      Type…   …or…   …to get      Some text in a paragraph.\nMore text in the next paragraph. Always use empty lines between paragraphs.      Some text in a paragraph.  More text in the next paragraph. Always use empty lines between paragraphs.    Italic   Italic   Italic    Bold   Bold   Bold    # Heading 1     Heading 1     ## Heading 2     Heading 2     ### Heading 3     Heading 3     (Go up to heading level 6 with ######)        Link text     Link text            \u0026lt;code\u0026gt;Inline code with backticks     Inline code with backticks    \u0026gt; Blockquote       Blockquote     - Things in - an unordered - list   * Things in * an unordered * list     Things in   an unordered   list      1. Things in 2. an ordered 3. list   1) Things in 2) an ordered 3) list    Things in   an ordered   list      Horizontal line ---   Horizontal line ***    Horizontal line      Math Basic math commands Markdown uses LaTeX to create fancy mathematical equations. There are like a billion little options and features available for math equations—you can find helpful examples of the the most common basic commands here. In this class, these will be the most common things you’ll use:\n  Description  Command  Output     Letters    Roman letters  a b c d e f  \\(a\\ b\\ c\\ d\\ e\\ f\\)    Greek letters (see this for all possible letters)     \\(\\alpha\\ \\beta\\ \\Gamma\\ \\gamma\\ \\Delta\\ \\delta\\ \\epsilon\\)    Letters will automatically be italicized and treated as math variables;\nif you want actual text in the math, use   Ew: Treatment =  Good:  =   Ew: \\(Treatment = \\beta\\)\nGood: \\(\\text{Treatment} = \\beta\\)    Extra spaces will automatically be removed; if you want a space, use    No space: x y Space: x y  No space: \\(x y\\) Space: \\(x \\ y\\)   Superscripts and subscripts    Use ^ to make one character superscripted.  x^2  \\(x^2\\)    Wrap the superscripted part in {} if there’s more than one character  x^{2+y}  \\(x^{2+y}\\)    Use _ to make one character subscripted  _1  \\(\\beta_1\\)    Wrap the subscripted part in {} if there’s more than one character  _{i, t}  \\(\\beta_{i, t}\\)    Use superscripts and subscripts simultaneously  _1^{}  \\(\\beta_1^{\\text{Treatment}}\\)    You can even nest them  x{2{2^2}}  \\(x^{2^{2^2}}\\)   Math operations    Addition  2 + 5 = 7  \\(2 + 5 = 7\\)    Subtraction  2 - 5 = -3  \\(2 + 5 = -3\\)    Multiplication  x y x y  \\(x \\times y\\) \\(x \\cdot y\\)    Division  8   \\(8 \\div 2\\)    Fractions    \\(\\frac{8}{2}\\)    Square roots; use [3] for other roots   = 9  = 3  \\(\\sqrt{81} = 9\\) \\(\\sqrt[3]{27} = 3\\)    Summation; use sub/superscripts for extra details  x _{n=1}^{}   \\(\\sum x\\) \\(\\sum_{n=1}^{\\infty} \\frac{1}{n}\\)    Products; use sub/superscripts for extra details  x _{n=1}^{5} n^2  \\(\\prod x\\) \\(\\prod_{n=1}^{5} n^2\\)    Integrals; use sub/superscripts for extra details  x^2 dx _{1}^{100} x^2 dx  \\(\\int x^2 \\ dx\\) \\(\\int_{1}^{100} x^2 \\ dx\\)   Extra symbols    Add a bar for things like averages  {x}  \\(\\bar{x}\\)    Use an overline for longer things  Ew: {abcdef} Good:   Ew: \\(\\bar{abcdef}\\) Good: \\(\\overline{abcdef}\\)    Add a hat for things like estimates    \\(\\hat{y}\\)    Use a wide hat for longer things  Ew:  Good:   Ew: \\(\\hat{abcdef}\\) Good: \\(\\widehat{abcdef}\\)    Use arrows for DAG-like things  Z Y X  \\(Z \\rightarrow Y \\leftarrow X\\)   Bonus fun    Use colors!; see here for more details and here for a list of color names   =   \\(\\color{red}{y}\\ \\color{black}{=}\\ \\color{blue}{\\beta_1 x_1}\\)      Using math inline You can use math in two different ways: inline or in a display block. To use math inline, wrap it in single dollar signs, like \\$y = mx + b\\$:\n     Type…   …to get      Based on the DAG, the regression model for estimating the effect of education on wages is \u0026dollar;\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\epsilon\u0026dollar;, or \u0026dollar;\\text{Wages} = \\beta_0 + \\beta_1 \\text{Education} + \\epsilon\u0026dollar;.   Based on the DAG, the regression model for estimating the effect of education on wages is \\(\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\epsilon\\), or \\(\\text{Wages} = \\beta_0 + \\beta_1 \\text{Education} + \\epsilon\\).      Using math in a block To put an equation on its own line in a display block, wrap it in double dollar signs, like this:\nType…\nThe quadratic equation was an important part of high school math: $$ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} $$ But now we just use computers to solve for $x$. …to get…\n The quadratic equation was an important part of high school math:\n\\[ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} \\]\nBut now we just use computers to solve for \\(x\\).\n  Dollar signs and math Because dollar signs are used to indicate math equations, you can’t just use dollar signs like normal if you’re writing about actual dollars. For instance, if you write This book costs \\$5.75 and this other costs \\$40, Markdown will treat everything that comes between the dollar signs as math, like so: “This book costs $5.75 and this other costs $40”.\nTo get around that, put a backslash (\\) in front of the dollar signs, so that This book costs \\\\\\$5.75 and this other costs \\\\\\$40 becomes “This book costs $5.75 and this other costs $40”.\n  Tables There are 4 different ways to hand-create tables in Markdown—I say “hand-create” because it’s normally way easier to use R to generate these things with packages like kableExtra (use kable()) or pander (use pandoc.table()). The two most common are simple tables and pipe tables. You should look at the full documentation here.\nFor simple tables, type…\n Right Left Center Default ------- ------ ---------- ------- 12 12 12 12 123 123 123 123 1 1 1 1 Table: Caption goes here …to get…\n Caption goes here  Right Left Center Default    12 12 12 12  123 123 123 123  1 1 1 1    For pipe tables, type…\n| Right | Left | Default | Center | |------:|:-----|---------|:------:| | 12 | 12 | 12 | 12 | | 123 | 123 | 123 | 123 | | 1 | 1 | 1 | 1 | Table: Caption goes here …to get…\n Caption goes here  Right Left Default Center    12 12 12 12  123 123 123 123  1 1 1 1     Footnotes There are two different ways to add footnotes (see here for complete documentation): regular and inline.\nRegular notes need (1) an identifier and (2) the actual note. The identifier can be whatever you want. Some people like to use numbers like [^1], but if you ever rearrange paragraphs or add notes before #1, the numbering will be wrong (in your Markdown file, not in the output; everything will be correct in the output). Because of that, I prefer to use some sort of text label:\nType…\nHere is a footnote reference[^1] and here is another [^note-on-dags]. [^1]: This is a note. [^note-on-dags]: DAGs are neat. And here\u0026#39;s more of the document. …to get…\n Here is a footnote reference1 and here is another.2\nAnd here’s more of the document.\n  This is a note.↩︎   DAGs are neat.↩︎     You can also use inline footnotes with ^[Text of the note goes here], which are often easier because you don’t need to worry about identifiers:\nType…\nCausal inference is neat.^[But it can be hard too!] …to get…\n Causal inference is neat.1\n  But it can be hard too!↩︎      Front matter You can include a special section at the top of a Markdown document that contains metadata (or data about your document) like the title, date, author, etc. This section uses a special simple syntax named YAML (or “YAML Ain’t Markup Language”) that follows this basic outline: setting: value for setting. Here’s an example YAML metadata section. Note that it must start and end with three dashes (---).\n--- title: Title of your document date: \u0026quot;January 13, 2020\u0026quot; author: \u0026quot;Your name\u0026quot; --- You can put the values inside quotes (like the date and name in the example above), or you can leave them outside of quotes (like the title in the example above). I typically use quotes just to be safe—if the value you’re using has a colon (:) in it, it’ll confuse Markdown since it’ll be something like title: My cool title: a subtitle, which has two colons. It’s better to do this:\n--- title: \u0026quot;My cool title: a subtitle\u0026quot; --- If you want to use quotes inside one of the values (e.g. your document is An evaluation of \"scare quotes\"), you can use single quotes instead:\n--- title: \u0026#39;An evaluation of \u0026quot;scare quotes\u0026quot;\u0026#39; ---  Citations One of the most powerful features of Markdown + pandoc is the ability to automatically cite things and generate bibliographies. to use citations, you need to create a BibTeX file (ends in .bib) that contains a database of the things you want to cite. You can do this with bibliography managers designed to work with BibTeX directly (like BibDesk on macOS), or you can use Zotero (macOS and Windows) to export a .bib file. You can download an example .bib file of all the readings from this class here.\nComplete details for using citations can be found here. In brief, you need to do three things:\nAdd a bibliography: entry to the YAML metadata:\n--- title: Title of your document date: \u0026quot;January 13, 2020\u0026quot; author: \u0026quot;Your name\u0026quot; bibliography: name_of_file.bib --- Choose a citation style based on a CSL file. The default is Chicago author-date, but you can choose from 2,000+ at this repository. Download the CSL file, put it in your project folder, and add an entry to the YAML metadata (or provide a URL to the online version):\n--- title: Title of your document date: \u0026quot;January 13, 2020\u0026quot; author: \u0026quot;Your name\u0026quot; bibliography: name_of_file.bib csl: \u0026quot;https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl\u0026quot; --- Some of the most common CSLs are:\n Chicago author-date Chicago note-bibliography Chicago full note-bibliography (no shortened notes or ibids) APA 7th edition MLA 8th edition  Cite things in your document. Check the documentation for full details of how to do this. Essentially, you use @citationkey inside square brackets ([]):\n    Type… …to get…    Causal inference is neat [@Rohrer:2018; @AngristPischke:2015]. Causal inference is neat (Rohrer 2018; Angrist and Pischke 2015).  Causal inference is neat [see @Rohrer:2018, p. 34; also @AngristPischke:2015, chapter 1]. Causal inference is neat (see Rohrer 2018, 34; also Angrist and Pischke 2015, chap. 1).  Angrist and Pischke say causal inference is neat [-@AngristPischke:2015; see also @Rohrer:2018]. Angrist and Pischke say causal inference is neat (2015; see also Rohrer 2018).  @AngristPischke:2015 [chapter 1] say causal inference is neat, and @Rohrer:2018 agrees. Angrist and Pischke (2015, chap. 1) say causal inference is neat, and Rohrer (2018) agrees.    After compiling, you should have a perfectly formatted bibliography added to the end of your document too:\n Angrist, Joshua D., and Jörn-Steffen Pischke. 2015. Mastering ’Metrics: The Path from Cause to Effect. Princeton, NJ: Princeton University Press.\nRohrer, Julia M. 2018. “Thinking Clearly About Correlations and Causation: Graphical Causal Models for Observational Data.” Advances in Methods and Practices in Psychological Science 1 (1): 27–42. https://doi.org/10.1177/2515245917745629.\n   Other references These websites have additional details and examples and practice tools:\n CommonMark’s Markdown tutorial: A quick interactive Markdown tutorial. Markdown tutorial: Another interactive tutorial to practice using Markdown. Markdown cheatsheet: Useful one-page reminder of Markdown syntax. The Plain Person’s Guide to Plain Text Social Science: A comprehensive explanation and tutorial about why you should write data-based reports in Markdown.   ","date":1578873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578873600,"objectID":"dcf6a5ae191a1cca4f4c8ff8ac114538","permalink":"https://dsba6010-spring2022.netlify.app/resource/markdown/","publishdate":"2020-01-13T00:00:00Z","relpermalink":"/resource/markdown/","section":"resource","summary":"Markdown is a special kind of markup language that lets you format text with simple syntax. You can then use a converter program like pandoc to convert Markdown into whatever format you want: HTML, PDF, Word, PowerPoint, etc.","tags":null,"title":"Using Markdown","type":"docs"},{"authors":null,"categories":null,"content":"   R Markdown is regular Markdown with R code and output sprinkled in. You can do everything you can with regular Markdown, but you can incorporate graphs, tables, and other R output directly in your document. You can create HTML, PDF, and Word documents, PowerPoint and HTML presentations, websites, books, and even interactive dashboards with R Markdown. This whole course website is created with R Markdown (and a package named blogdown).\nThe documentation for R Markdown is extremely comprehensive, and their tutorials and cheatsheets are excellent—rely on those.\nHere are the most important things you’ll need to know about R Markdown in this class:\nKey terms  Document: A Markdown file where you type stuff\n Chunk: A piece of R code that is included in your document. It looks like this:\n```{r} # Code goes here ``` There must be an empty line before and after the chunk. The final three backticks must be the only thing on the line—if you add more text, or if you forget to add the backticks, or accidentally delete the backticks, your document will not knit correctly.\n Knit: When you “knit” a document, R runs each of the chunks sequentially and converts the output of each chunk into Markdown. R then runs the knitted document through pandoc to convert it to HTML or PDF or Word (or whatever output you’ve selected).\nYou can knit by clicking on the “Knit” button at the top of the editor window, or by pressing ⌘⇧K on macOS or control + shift + K on Windows.\n   Add chunks There are three ways to insert chunks:\n Press ⌘⌥I on macOS or control + alt + I on Windows\n Click on the “Insert” button at the top of the editor window\n Manually type all the backticks and curly braces (don’t do this)\n   Chunk names You can add names to chunks to make it easier to navigate your document. If you click on the little dropdown menu at the bottom of your editor in RStudio, you can see a table of contents that shows all the headings and chunks. If you name chunks, they’ll appear in the list. If you don’t include a name, the chunk will still show up, but you won’t know what it does.\nTo add a name, include it immediately after the {r in the first line of the chunk. Names cannot contain spaces, but they can contain underscores and dashes. All chunk names in your document must be unique.\n```{r name-of-this-chunk} # Code goes here ```  Chunk options There are a bunch of different options you can set for each chunk. You can see a complete list in the RMarkdown Reference Guide or at knitr’s website.\nOptions go inside the {r} section of the chunk:\n```{r name-of-this-chunk, warning=FALSE, message=FALSE} # Code goes here ``` The most common chunk options are these:\n fig.width=5 and fig.height=3 (or whatever number you want): Set the dimensions for figures echo=FALSE: The code is not shown in the final document, but the results are message=FALSE: Any messages that R generates (like all the notes that appear after you load a package) are omitted warning=FALSE: Any warnings that R generates are omitted include=FALSE: The chunk still runs, but the code and results are not included in the final document  You can also set chunk options by clicking on the little gear icon in the top right corner of any chunk:\n Inline chunks You can also include R output directly in your text, which is really helpful if you want to report numbers from your analysis. To do this, use `r r_code_here`.\nIt’s generally easiest to calculate numbers in a regular chunk beforehand and then use an inline chunk to display the value in your text. For instance, this document…\n```{r find-avg-mpg, echo=FALSE} avg_mpg \u0026lt;- mean(mtcars$mpg) ``` The average fuel efficiency for cars from 1974 was `r round(avg_mpg, 1)` miles per gallon. … would knit into this:\n The average fuel efficiency for cars from 1974 was 20.1 miles per gallon.\n  Output formats You can specify what kind of document you create when you knit in the YAML front matter.\ntitle: \u0026quot;My document\u0026quot; output: html_document: default pdf_document: default word_document: default You can also click on the down arrow on the “Knit” button to choose the output and generate the appropriate YAML. If you click on the gear icon next to the “Knit” button and choose “Output options”, you change settings for each specific output type, like default figure dimensions or whether or not a table of contents is included.\nThe first output type listed under output: will be what is generated when you click on the “Knit” button or press the keyboard shortcut (⌘⇧K on macOS; control + shift + K on Windows). If you choose a different output with the “Knit” button menu, that output will be moved to the top of the output section.\nThe indentation of the YAML section matters, especially when you have settings nested under each output type. Here’s what a typical output section might look like:\n--- title: \u0026quot;My document\u0026quot; author: \u0026quot;My name\u0026quot; date: \u0026quot;January 13, 2020\u0026quot; output: html_document: toc: yes fig_caption: yes fig_height: 8 fig_width: 10 pdf_document: latex_engine: xelatex # More modern PDF typesetting engine toc: yes word_document: toc: yes fig_caption: yes fig_height: 4 fig_width: 5 ---  ","date":1578873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578873600,"objectID":"00c0b36df90b91640842af65d1311657","permalink":"https://dsba6010-spring2022.netlify.app/resource/rmarkdown/","publishdate":"2020-01-13T00:00:00Z","relpermalink":"/resource/rmarkdown/","section":"resource","summary":"R Markdown is regular Markdown with R code and output sprinkled in. You can do everything you can with regular Markdown, but you can incorporate graphs, tables, and other R output directly in your document.","tags":null,"title":"Using R Markdown","type":"docs"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://dsba6010-spring2022.netlify.app/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]